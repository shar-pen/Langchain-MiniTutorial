{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VectorStore-backed Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**基于VectorStore的检索器** 是一种文档检索系统，它利用向量存储根据文档的向量表示来进行搜索。这种方法使得基于相似度的搜索变得高效，特别适用于处理非结构化数据。\n",
    "\n",
    "RAG系统中的文档搜索和响应生成步骤包括：\n",
    "\n",
    "1. **文档加载**：导入原始文档。\n",
    "2. **文本切分**：将文本切分成可管理的块。\n",
    "3. **向量嵌入**：使用嵌入模型将文本转换为数值向量。\n",
    "4. **存储到向量数据库**：将生成的嵌入向量存储到向量数据库中，以便高效检索。\n",
    "\n",
    "在查询阶段：\n",
    "- 流程：用户查询 → 嵌入 → 在向量存储中搜索 → 检索相关块 → LLM生成响应\n",
    "- 用户的查询被转化为一个嵌入向量，使用嵌入模型。\n",
    "- 该查询嵌入向量与向量数据库中存储的文档向量进行比较，以 **检索最相关的结果**。\n",
    "- 检索到的文档块被传递给大语言模型（LLM），该模型基于检索到的信息生成最终响应。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "embed_dim = len(openai_embedding.embed_query(\"hello world\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",\n",
    "    \"AI can analyze medical images to detect conditions like cancer.\",\n",
    "    \"Machine learning predicts patient outcomes based on health data.\",\n",
    "    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",\n",
    "    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",\n",
    "    \"AI automates administrative tasks, saving time for healthcare workers.\",\n",
    "    \"NLP extracts insights from electronic health records for better care.\",\n",
    "    \"AI chatbots help with patient assessments and symptom checking.\",\n",
    "    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",\n",
    "    \"AI optimizes hospital operations and reduces healthcare costs.\"\n",
    "]\n",
    "\n",
    "documents = [\n",
    "\tDocument(text, metadata={\"source\":text})\n",
    "\tfor text in texts\n",
    "]\n",
    "\n",
    "db = FAISS.from_documents(documents, openai_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦向量数据库创建完成，就可以使用检索方法，如 **相似度搜索** 和 **最大边际相关性（MMR）**，加载并查询数据库，从中搜索相关的文本。\n",
    "\n",
    "`as_retriever` 方法允许你将一个向量数据库转换为一个检索器，从而实现从向量库中高效地搜索和检索文档。\n",
    "\n",
    "**工作原理**：\n",
    "* `as_retriever()` 方法将一个向量库（如 FAISS）转换为一个检索器对象，使其与 LangChain 的检索工作流兼容。\n",
    "* 这个检索器可以直接用于 RAG 流水线，或与大型语言模型（LLM）结合，用于构建智能搜索系统。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **高级检索器配置**\n",
    "\n",
    "`as_retriever` 方法允许你配置高级检索策略，如 **相似度搜索**、**最大边际相关性（MMR）** 和 **基于相似度分数阈值的过滤**。\n",
    "\n",
    "**参数：**\n",
    "\n",
    "- `**kwargs`：传递给检索函数的关键字参数：\n",
    "   - `search_type`：指定搜索方法。\n",
    "     - `\"similarity\"`：基于余弦相似度返回最相关的文档。\n",
    "     - `\"mmr\"`：利用最大边际相关性算法，平衡 **相关性** 和 **多样性**。\n",
    "     - `\"similarity_score_threshold\"`：返回相似度分数超过指定阈值的文档。\n",
    "   - `search_kwargs`：其他用于微调结果的搜索选项：\n",
    "     - `k`：返回的文档数量（默认值：`4`）。\n",
    "     - `score_threshold`：用于 `\"similarity_score_threshold\"` 搜索类型的最小相似度分数（例如：`0.8`）。\n",
    "     - `fetch_k`：在 MMR 搜索过程中最初检索的文档数量（默认值：`20`）。\n",
    "     - `lambda_mult`：控制 MMR 结果中的多样性（`0` = 最大多样性，`1` = 最大相关性，默认值：`0.5`）。\n",
    "     - `filter`：用于选择性文档检索的元数据过滤。\n",
    "\n",
    "**返回值：**\n",
    "\n",
    "- `VectorStoreRetriever`：初始化后的检索器对象，可以直接用于文档搜索任务。\n",
    "\n",
    "**注意事项：**\n",
    "- 支持多种搜索策略（`similarity`、`MMR`、`similarity_score_threshold`）。\n",
    "- MMR 通过减少结果中的冗余，提升结果多样性同时保持相关性。\n",
    "- 元数据过滤使得根据文档属性选择性地检索文档成为可能。\n",
    "- `tags` 参数可以用于给检索器加标签，以便更好地组织和识别。\n",
    "\n",
    "**警告：**\n",
    "- 使用 MMR 时的多样性控制：\n",
    "  - 小心调整 `fetch_k`（最初检索的文档数量）和 `lambda_mult`（多样性控制因子）以获得最佳平衡。\n",
    "  - `lambda_mult`：\n",
    "    - 较低值（< 0.5）→ 优先考虑多样性。\n",
    "    - 较高值（> 0.5）→ 优先考虑相关性。\n",
    "  - 为有效的多样性控制，设置 `fetch_k` 大于 `k`。\n",
    "- 阈值设置：\n",
    "  - 使用较高的 `score_threshold`（例如 0.95）可能会导致没有结果。\n",
    "- 元数据过滤：\n",
    "  - 在应用过滤器之前，确保元数据结构已经定义好。\n",
    "- 平衡配置：\n",
    "  - 为了获得最佳的检索性能，保持 `search_type` 和 `search_kwargs` 设置之间的适当平衡。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No relevant docs were retrieved using the relevance score threshold 0.5\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\", \n",
    "    search_kwargs={\n",
    "        \"k\": 5,  # Return the top 5 most relevant documents\n",
    "        \"score_threshold\": 0.5  # Only return documents with a similarity score of 0.4 or higher\n",
    "    }\n",
    ")\n",
    "\n",
    "query = \"How does AI improve healthcare?\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Display search results\n",
    "for doc in results:\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 检索器的 `invoke()` 方法\n",
    "\n",
    "`invoke()` 方法是与检索器交互的主要入口点。它用于根据给定的查询搜索并检索相关的文档。\n",
    "\n",
    "**工作原理：**\n",
    "1. **查询提交**：用户提交查询字符串作为输入。\n",
    "2. **嵌入生成**：如果需要，查询会被转换成向量表示。\n",
    "3. **搜索过程**：检索器使用指定的搜索策略（如相似度、MMR 等）在向量数据库中进行搜索。\n",
    "4. **结果返回**：该方法返回一组相关的文档片段。\n",
    "\n",
    "**参数：**\n",
    "- `input`（必需）：\n",
    "   - 用户提供的查询字符串。\n",
    "   - 查询会被转换成向量，并与存储的文档向量进行相似度比较，以进行基于相似度的检索。\n",
    "\n",
    "- `config`（可选）：\n",
    "   - 允许对检索过程进行细粒度控制。\n",
    "   - 可用于指定 **标签、元数据插入和搜索策略**。\n",
    "\n",
    "- `**kwargs`（可选）：\n",
    "   - 允许直接传递 `search_kwargs` 进行高级配置。\n",
    "   - 示例选项包括：\n",
    "     - `k`：返回的文档数量。\n",
    "     - `score_threshold`：文档被包括的最低相似度分数。\n",
    "     - `fetch_k`：MMR 搜索中最初检索的文档数量。\n",
    "\n",
    "**返回值：**\n",
    "- `List[Document]`：\n",
    "   - 返回包含检索到的文本和元数据的文档对象列表。\n",
    "   - 每个文档对象包括：\n",
    "     - `page_content`：文档的主要内容。\n",
    "     - `metadata`：与文档相关联的元数据（例如，来源、标签）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用例 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning predicts patient outcomes based on health data.\n",
      "=========================================================\n",
      "AI monitors patients remotely, enabling proactive care for chronic diseases.\n",
      "=========================================================\n",
      "AI chatbots help with patient assessments and symptom checking.\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.invoke(\"What is an embedding?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**用例 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine learning predicts patient outcomes based on health data.\n",
      "=========================================================\n",
      "AI monitors patients remotely, enabling proactive care for chronic diseases.\n",
      "=========================================================\n",
      "AI chatbots help with patient assessments and symptom checking.\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# search options: top 5 results with a similarity score ≥ 0.7\n",
    "docs = retriever.invoke(\n",
    "    \"What is a vector database?\",\n",
    "    search_kwargs={\"k\": 5, \"score_threshold\": 0.7}\n",
    ")\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 最大边际相关性 (MMR)\n",
    "\n",
    "**最大边际相关性 (MMR)** 搜索方法是一种文档检索算法，旨在通过平衡相关性和多样性来减少冗余，从而返回结果时提高多样性。\n",
    "\n",
    "**MMR 的工作原理：**\n",
    "与仅根据相似度分数返回最相关文档的基本相似度搜索不同，MMR 考虑了两个关键因素：\n",
    "1. **相关性**：衡量文档与用户查询的匹配程度。\n",
    "2. **多样性**：确保检索到的文档彼此不同，避免重复的结果。\n",
    "\n",
    "**关键参数：**\n",
    "- `search_type=\"mmr\"`：启用 MMR 检索策略。\n",
    "- `k`：应用多样性过滤后返回的文档数量（默认值：`4`）。\n",
    "- `fetch_k`：应用多样性过滤前最初检索的文档数量（默认值：`20`）。\n",
    "- `lambda_mult`：多样性控制因子（`0 = 最大多样性`，`1 = 最大相关性`，默认值：`0.5`）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似度分数阈值搜索\n",
    "\n",
    "**相似度分数阈值搜索**是一种检索方法，只有当文档的相似度分数超过预定义的阈值时才会返回。该方法有助于筛选出低相关性的结果，确保返回的文档与查询高度相关。\n",
    "\n",
    "**关键特性：**\n",
    "- **相关性过滤**：仅返回相似度分数高于指定阈值的文档。\n",
    "- **可调精度**：通过 `score_threshold` 参数调整阈值。\n",
    "- **启用搜索类型**：通过设置 `search_type=\"similarity_score_threshold\"` 启用此搜索方法。\n",
    "\n",
    "这种搜索方法非常适用于需要**高度精确**结果的任务，例如事实核查或回答技术性查询。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 配置 `top_k`（调整返回文档的数量）\n",
    "\n",
    "- 参数 `k` 指定在向量搜索过程中返回的文档数量。它决定了从向量数据库中检索到的 **排名最高**（基于相似度分数）的文档数量。\n",
    "\n",
    "- 通过在 `search_kwargs` 中设置 `k` 值，可以调整检索到的文档数量。\n",
    "- 例如，设置 `k=1` 将仅返回 **最相关的 1 篇文档**，该文档基于相似度排序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ContextualCompressionRetriever` 是 LangChain 中的一种强大工具，旨在通过根据上下文压缩检索到的文档来优化检索过程。这个检索器特别适用于需要对大量数据进行动态总结或过滤的场景，确保只有最相关的信息传递到后续处理步骤。\n",
    "\n",
    "`ContextualCompressionRetriever` 的主要特点包括：\n",
    "\n",
    "- **上下文感知压缩**：文档会根据特定的上下文或查询进行压缩，确保相关性并减少冗余。\n",
    "- **灵活的集成**：与其他 LangChain 组件无缝工作，便于集成到现有的管道中。\n",
    "- **可定制的压缩**：支持使用不同的压缩技术，包括摘要模型和基于嵌入的方法，来根据需求定制检索过程。\n",
    "\n",
    "`ContextualCompressionRetriever` 特别适用于以下应用：\n",
    "\n",
    "- 为问答系统总结大量数据。\n",
    "- 通过提供简洁且相关的回答来提升聊天机器人性能。\n",
    "- 提高文档密集型任务（如法律分析或学术研究）的效率。\n",
    "\n",
    "通过使用这个检索器，开发者可以显著减少计算开销，并提高提供给最终用户的信息质量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 419, which is longer than the specified 400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Semantic Search\n",
      "document 2:\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "document 3:\n",
      "\n",
      "Definition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\n",
      "Example: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "Tokenizer\n",
      "document 4:\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\n",
      "Example: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "VectorStore\n",
      "document 5:\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector form. It is used for tasks like retrieval, classification, and other data analysis.\n",
      "Example: Word embedding vectors can be stored in a database for quick access.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "\n",
      "SQL\n",
      "document 6:\n",
      "\n",
      "Definition: SQL (Structured Query Language) is a programming language for managing data in databases. It supports operations like querying, modifying, inserting, and deleting data.\n",
      "Example: SELECT * FROM users WHERE age > 18; retrieves information about users older than 18.\n",
      "Related Keywords: Database, Query, Data Management\n",
      "\n",
      "CSV\n",
      "document 7:\n",
      "\n",
      "Definition: CSV (Comma-Separated Values) is a file format for storing data where each value is separated by a comma. It is often used for simple data storage and exchange in tabular form.\n",
      "Example: A CSV file with headers “Name, Age, Job” might contain data like “John Doe, 30, Developer”.\n",
      "Related Keywords: File Format, Data Handling, Data Exchange\n",
      "\n",
      "JSON\n",
      "document 8:\n",
      "\n",
      "Definition: JSON (JavaScript Object Notation) is a lightweight data exchange format that represents data objects in a human- and machine-readable text format.\n",
      "Example: {\"name\": \"John Doe\", \"age\": 30, \"job\": \"Developer\"} is an example of JSON data.\n",
      "Related Keywords: Data Exchange, Web Development, API\n",
      "\n",
      "Transformer\n",
      "document 9:\n",
      "\n",
      "Definition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.\n",
      "Example: Google Translate uses transformer models to perform translations between languages.\n",
      "Related Keywords: Deep Learning, Natural Language Processing, Attention\n",
      "\n",
      "HuggingFace\n",
      "document 10:\n",
      "\n",
      "Definition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making NLP tasks more accessible to researchers and developers.\n",
      "Example: HuggingFace’s Transformers library can be used for tasks like sentiment analysis and text generation.\n",
      "Related Keywords: Natural Language Processing, Deep Learning, Library\n",
      "\n",
      "Digital Transformation\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "# 1. Generate Loader to lthe text file using TextLoader\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\\\n",
    "\n",
    "# 2. Generate text chunks using CharacterTextSplitter and split the text into chunks of 300 characters with no overlap.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=400, chunk_overlap=0)\n",
    "texts = loader.load_and_split(text_splitter)\n",
    "\n",
    "# 3. Generate vector store using FAISS and convert it to retriever\n",
    "embedder = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "retriever = FAISS.from_documents(texts, embedder).as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# 4. Query the retriever to find relevant documents\n",
    "docs = retriever.invoke(\"What is the definition of Multimodal?\")\n",
    "\n",
    "# 5. Print the relevant documents\n",
    "for i, d in enumerate(docs):\n",
    "\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 `LLMChainExtractor` 创建的 `DocumentCompressor` 正是应用于检索器的，即 `ContextualCompressionRetriever`。\n",
    "\n",
    "`ContextualCompressionRetriever` 会通过去除无关信息并专注于最相关的信息来压缩文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLMChainFilter\n",
    "\n",
    "`LLMChainFilter` 是一个简单但强大的压缩器，它使用 LLM 链来决定从最初检索到的文档中哪些应该被过滤，哪些应该被返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "document 1:\n",
      "\n",
      "Semantic Search\n",
      "document 2:\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "document 3:\n",
      "\n",
      "Definition: A token refers to a smaller unit of text obtained by splitting a larger text. It can be a word, sentence, or phrase.\n",
      "Example: The sentence “I go to school” can be split into tokens: “I”, “go”, “to”, “school”.\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "Tokenizer\n",
      "document 4:\n",
      "\n",
      "Definition: A tokenizer is a tool that splits text data into tokens. It is commonly used in natural language processing for data preprocessing.\n",
      "Example: The sentence “I love programming.” can be tokenized into [“I”, “love”, “programming”, “.”].\n",
      "Related Keywords: Tokenization, Natural Language Processing, Parsing\n",
      "\n",
      "VectorStore\n",
      "document 5:\n",
      "\n",
      "Definition: A vector store is a system for storing data in vector form. It is used for tasks like retrieval, classification, and other data analysis.\n",
      "Example: Word embedding vectors can be stored in a database for quick access.\n",
      "Related Keywords: Embedding, Database, Vectorization\n",
      "\n",
      "SQL\n",
      "document 6:\n",
      "\n",
      "Definition: SQL (Structured Query Language) is a programming language for managing data in databases. It supports operations like querying, modifying, inserting, and deleting data.\n",
      "Example: SELECT * FROM users WHERE age > 18; retrieves information about users older than 18.\n",
      "Related Keywords: Database, Query, Data Management\n",
      "\n",
      "CSV\n",
      "document 7:\n",
      "\n",
      "Definition: CSV (Comma-Separated Values) is a file format for storing data where each value is separated by a comma. It is often used for simple data storage and exchange in tabular form.\n",
      "Example: A CSV file with headers “Name, Age, Job” might contain data like “John Doe, 30, Developer”.\n",
      "Related Keywords: File Format, Data Handling, Data Exchange\n",
      "\n",
      "JSON\n",
      "document 8:\n",
      "\n",
      "Definition: JSON (JavaScript Object Notation) is a lightweight data exchange format that represents data objects in a human- and machine-readable text format.\n",
      "Example: {\"name\": \"John Doe\", \"age\": 30, \"job\": \"Developer\"} is an example of JSON data.\n",
      "Related Keywords: Data Exchange, Web Development, API\n",
      "\n",
      "Transformer\n",
      "document 9:\n",
      "\n",
      "Definition: A transformer is a type of deep learning model used in natural language processing for tasks like translation, summarization, and text generation. It is based on the attention mechanism.\n",
      "Example: Google Translate uses transformer models to perform translations between languages.\n",
      "Related Keywords: Deep Learning, Natural Language Processing, Attention\n",
      "\n",
      "HuggingFace\n",
      "document 10:\n",
      "\n",
      "Definition: HuggingFace is a library that provides pre-trained models and tools for natural language processing, making NLP tasks more accessible to researchers and developers.\n",
      "Example: HuggingFace’s Transformers library can be used for tasks like sentiment analysis and text generation.\n",
      "Related Keywords: Natural Language Processing, Deep Learning, Library\n",
      "\n",
      "Digital Transformation\n",
      "==============================================================\n",
      "===============After applying LLMChainExtractor===============\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Before applying ContextualCompressionRetriever\n",
    "docs = retriever.invoke(\"What is the definition of Multimodal?\")\n",
    "for i, d in enumerate(docs):\n",
    "\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)\n",
    "print(\"=\"*62)\n",
    "print(\"=\"*15 + \"After applying LLMChainExtractor\" + \"=\"*15)\n",
    "\n",
    "\n",
    "# After applying ContextualCompressionRetriever\n",
    "# 1. Generate LLM\n",
    "llm = ChatOpenAI(\n",
    "\tbase_url='http://localhost:5551/v1',\n",
    "\tapi_key='EMPTY',\n",
    "\tmodel_name='Qwen2.5-7B-Instruct',\n",
    "\ttemperature=0.2,\n",
    ")\n",
    "\n",
    "\n",
    "# 2. Generate compressor using LLMChainExtractor\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "\n",
    "# 3. Generate compression retriever using ContextualCompressionRetriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever,\n",
    ")\n",
    "\n",
    "# 4. Query the compression retriever to find relevant documents\n",
    "compressed_docs = (\n",
    "    compression_retriever.invoke( \n",
    "        \"What is the definition of Multimodal?\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 5. Print the relevant documents\n",
    "for i, d in enumerate(compressed_docs):\n",
    "\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大模型把无关内容都过滤了, 虽然我 embedding 很拉, 没能抽到相关内容  \n",
    "以下是一个过滤效果的展示, 把定义成功保留, 示例被过滤掉"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multimodal\n",
      "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n"
     ]
    }
   ],
   "source": [
    "text = \\\n",
    "\"\"\"\n",
    "Multimodal\n",
    "Definition: Multimodal refers to the technology that combines multiple types of data modes (e.g., text, images, sound) to process and extract richer and more accurate information or predictions.\n",
    "Example: A system that analyzes both images and descriptive text to perform more accurate image classification is an example of multimodal technology.\n",
    "Relate\n",
    "\"\"\"\n",
    "docs = [Document(text)]\n",
    "query = \"What is the definition of Multimodal?\"\n",
    "compressed_docs = compressor.compress_documents(docs, query)\n",
    "print(compressed_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源码分析\n",
    "\n",
    "这是 `ContextualCompressionRetriever` 的检索函数 `_get_relevant_documents`的关键代码:\n",
    "```python\n",
    "\tdocs = self.base_retriever.invoke(\n",
    "\t\tquery, config={\"callbacks\": run_manager.get_child()}, **kwargs\n",
    "\t)\n",
    "\tif docs:\n",
    "\t\tcompressed_docs = self.base_compressor.compress_documents(\n",
    "\t\t\tdocs, query, callbacks=run_manager.get_child()\n",
    "\t\t)\n",
    "\t\treturn list(compressed_docs)\n",
    "\telse:\n",
    "\t\treturn []\n",
    "```\n",
    "首先还是 base_retriever 支持返回检索结果, 再接过 base_compressor 压缩\n",
    "\n",
    "这是 base_compresser 类 `LLMChainExtractor` 的 `compress_documents`函数关键部分:\n",
    "```python\n",
    "\tcompressed_docs = []\n",
    "\tfor doc in documents:\n",
    "\t\t_input = self.get_input(query, doc) # 产生 {\"question\": query, \"context\": doc.page_content}\n",
    "\t\toutput_ = self.llm_chain.invoke(_input, config={\"callbacks\": callbacks}) # 调用大模型抽取内容\n",
    "\t\tif isinstance(self.llm_chain, LLMChain):\n",
    "\t\t\toutput = output_[self.llm_chain.output_key]\n",
    "\t\t\tif self.llm_chain.prompt.output_parser is not None:\n",
    "\t\t\t\toutput = self.llm_chain.prompt.output_parser.parse(output)\n",
    "\t\telse:\n",
    "\t\t\toutput = output_\n",
    "\t\tif len(output) == 0:\n",
    "\t\t\tcontinue\n",
    "\t\tcompressed_docs.append(\n",
    "\t\t\tDocument(page_content=cast(str, output), metadata=doc.metadata)\n",
    "\t\t)\n",
    "\treturn compressed_docs\n",
    "```\n",
    "\n",
    "这是调用大模型抽取内容的 prompt 模板\n",
    "```python\n",
    "\"\"\"\n",
    "Given the following question and context, extract any part of the context *AS IS* that is relevant to answer the question. If none of the context is relevant return {no_output_str}. \n",
    "\n",
    "Remember, *DO NOT* edit the extracted parts of the context.\n",
    "\n",
    "> Question: {{question}}\n",
    "> Context:\n",
    ">>>\n",
    "{{context}}\n",
    ">>>\n",
    "Extracted relevant parts:\n",
    "\"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EmbeddingsFilter\n",
    "\n",
    "对每个检索到的文档执行额外的 LLM 调用既昂贵又缓慢。  \n",
    "`EmbeddingsFilter` 提供了一个更经济且更快速的选项，通过嵌入文档和查询，只返回那些与查询的嵌入相似度足够高的文档。\n",
    "\n",
    "这种方法在保持搜索结果相关性的同时，节省了计算成本和时间。  \n",
    "该过程涉及使用 `EmbeddingsFilter` 和 `ContextualCompressionRetriever` 压缩并检索相关文档。\n",
    "\n",
    "- `EmbeddingsFilter` 用于过滤超过指定相似度阈值（0.86）的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 1. Generate embeddings using OpenAIEmbeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "# 2. Generate EmbedingsFilter object that has similarity threshold of 0.86\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=embeddings, similarity_threshold=0.86)\n",
    "\n",
    "# 3. Generate ContextualCompressionRetriever object using EmbeddingsFilter and retriever\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=embeddings_filter, \n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# 4. Query the compression retriever to find relevant documents\n",
    "compressed_docs = compression_retriever.invoke(\n",
    "    \"What is the definition of Multimodal?\"\n",
    ")\n",
    "\n",
    "# 5. Print the relevant documents\n",
    "for i, d in enumerate(compressed_docs):\n",
    "\tprint(f\"document {i+1}:\\n\\n\" + d.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个方法也只是将 base_retriever 的返回结果经过 EmbeddingsFilter 的相似度阈值过滤, 可以选择更强的 embedding model 来强化相似度准确度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Retriever 多路召回\n",
    "\n",
    "`EnsembleRetriever` 集成了稀疏和密集检索算法的优点，通过使用权重和运行时配置来定制性能。\n",
    "\n",
    "**关键特点**\n",
    "1. 集成多个检索器：接受不同类型的检索器作为输入并结合结果。\n",
    "2. 结果重新排序：使用[倒排排名融合](https://plg.uwaterloo.ca/~gvcormac/cormacksigir09-rrf.pdf)算法重新排序结果。\n",
    "3. 混合检索：主要使用`稀疏检索器`（例如 BM25）和`密集检索器`（例如 嵌入相似度）相结合。\n",
    "\n",
    "**优势**\n",
    "- 稀疏检索器：有效进行基于关键词的检索。\n",
    "- 密集检索器：有效进行基于语义相似度的检索。\n",
    "\n",
    "由于这些互补特性，`EnsembleRetriever` 可以在各种检索场景中提供更好的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# list sample documents\n",
    "doc_list = [\n",
    "    \"I like apples\",\n",
    "    \"I like apple company\",\n",
    "    \"I like apple's iphone\",\n",
    "    \"Apple is my favorite company\",\n",
    "    \"I like apple's ipad\",\n",
    "    \"I like apple's macbook\",\n",
    "]\n",
    "\n",
    "# Initialize the bm25 retriever and faiss retriever.\n",
    "bm25_retriever = BM25Retriever.from_texts(\n",
    "    doc_list,\n",
    ")\n",
    "bm25_retriever.k = 2  # Set the number of search results for BM25Retriever to 1.\n",
    "\n",
    "embedding = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    "\t)\n",
    "\n",
    "faiss_vectorstore = FAISS.from_texts(\n",
    "    doc_list,\n",
    "    embedding,\n",
    ")\n",
    "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# Initialize the ensemble retriever.\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, faiss_retriever],\n",
    "    weights=[0.7, 0.3],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ensemble Retriever]\n",
      "Content: Apple is my favorite company\n",
      "\n",
      "Content: I like apple company\n",
      "\n",
      "Content: I like apples\n",
      "\n",
      "[BM25 Retriever]\n",
      "Content: Apple is my favorite company\n",
      "\n",
      "Content: I like apple company\n",
      "\n",
      "[FAISS Retriever]\n",
      "Content: Apple is my favorite company\n",
      "\n",
      "Content: I like apples\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the search results document.\n",
    "query = \"my favorite fruit is apple\"\n",
    "ensemble_result = ensemble_retriever.invoke(query)\n",
    "bm25_result = bm25_retriever.invoke(query)\n",
    "faiss_result = faiss_retriever.invoke(query)\n",
    "\n",
    "# Output the fetched documents.\n",
    "print(\"[Ensemble Retriever]\")\n",
    "for doc in ensemble_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[BM25 Retriever]\")\n",
    "for doc in bm25_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()\n",
    "\n",
    "print(\"[FAISS Retriever]\")\n",
    "for doc in faiss_result:\n",
    "    print(f\"Content: {doc.page_content}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源码分析\n",
    "\n",
    "`EnsembleRetriever` 的 `rank_fusion` 函数:\n",
    "```python\n",
    "retriever_docs = [\n",
    "\tretriever.invoke(\n",
    "\t\tquery,\n",
    "\t\tpatch_config(\n",
    "\t\t\tconfig, callbacks=run_manager.get_child(tag=f\"retriever_{i + 1}\")\n",
    "\t\t),\n",
    "\t)\n",
    "\tfor i, retriever in enumerate(self.retrievers)\n",
    "]\n",
    "\n",
    "# Enforce that retrieved docs are Documents for each list in retriever_docs\n",
    "for i in range(len(retriever_docs)):\n",
    "\tretriever_docs[i] = [\n",
    "\t\tDocument(page_content=cast(str, doc)) if isinstance(doc, str) else doc\n",
    "\t\tfor doc in retriever_docs[i]\n",
    "\t]\n",
    "\n",
    "# apply rank fusion\n",
    "fused_documents = self.weighted_reciprocal_rank(retriever_docs)\n",
    "```\n",
    "\n",
    "每个 retriever 单独调用, 返回多组 Documents, 再经过 `weighted_reciprocal_rank`:\n",
    "\n",
    "```python\n",
    "rrf_score: Dict[str, float] = defaultdict(float)\n",
    "for doc_list, weight in zip(doc_lists, self.weights):\n",
    "\tfor rank, doc in enumerate(doc_list, start=1):\n",
    "\t\trrf_score[\n",
    "\t\t\t(\n",
    "\t\t\t\tdoc.page_content\n",
    "\t\t\t\tif self.id_key is None\n",
    "\t\t\t\telse doc.metadata[self.id_key]\n",
    "\t\t\t)\n",
    "\t\t] += weight / (rank + self.c)\n",
    "\n",
    "# Docs are deduplicated by their contents then sorted by their scores\n",
    "all_docs = chain.from_iterable(doc_lists)\n",
    "sorted_docs = sorted(\n",
    "\tunique_by_key(\n",
    "\t\tall_docs,\n",
    "\t\tlambda doc: (\n",
    "\t\t\tdoc.page_content\n",
    "\t\t\tif self.id_key is None\n",
    "\t\t\telse doc.metadata[self.id_key]\n",
    "\t\t),\n",
    "\t),\n",
    "\treverse=True,\n",
    "\tkey=lambda doc: rrf_score[\n",
    "\t\tdoc.page_content if self.id_key is None else doc.metadata[self.id_key]\n",
    "\t],\n",
    ")\n",
    "```\n",
    "基于 weights 对 Documents 重排序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Context Reorder\n",
    "\n",
    "无论模型的架构如何，当检索的文档超过 10 个时，性能都会显著下降。\n",
    "\n",
    "简单来说，当模型需要在长上下文的中间部分访问相关信息时，它往往会忽视提供的文档。\n",
    "\n",
    "更多细节，请参阅以下论文：\n",
    "\n",
    "- [https://arxiv.org/abs/2307.03172](https://arxiv.org/abs/2307.03172)\n",
    "\n",
    "为了避免这个问题，您可以在检索后重新排序文档，从而防止性能下降。\n",
    "\n",
    "可以创建一个检索器，它使用 Chroma 向量数据库存储和搜索文本数据。然后，使用检索器的 `invoke` 方法，针对给定的查询搜索出高度相关的文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Get embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    "\t)\n",
    "\n",
    "texts = [\n",
    "    \"This is just a random text I wrote.\",\n",
    "    \"ChatGPT, an AI designed to converse with users, can answer various questions.\",\n",
    "    \"iPhone, iPad, MacBook are representative products released by Apple.\",\n",
    "    \"ChatGPT was developed by OpenAI and is continuously being improved.\",\n",
    "    \"ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.\",\n",
    "    \"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\",\n",
    "    \"ChatGPT can be used to solve complex problems or suggest creative ideas.\",\n",
    "    \"Bitcoin is also called digital gold and is gaining popularity as a store of value.\",\n",
    "    \"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\",\n",
    "    \"The FIFA World Cup is held every four years and is the biggest event in international football.\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Create a retriever (Set K to 10)\n",
    "retriever = Chroma.from_texts(texts, embedding=embeddings).as_retriever(\n",
    "    search_kwargs={\"k\": 10}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='Bitcoin is also called digital gold and is gaining popularity as a store of value.'),\n",
       " Document(metadata={}, page_content='The FIFA World Cup is held every four years and is the biggest event in international football.'),\n",
       " Document(metadata={}, page_content=\"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\"),\n",
       " Document(metadata={}, page_content='iPhone, iPad, MacBook are representative products released by Apple.'),\n",
       " Document(metadata={}, page_content='This is just a random text I wrote.'),\n",
       " Document(metadata={}, page_content='ChatGPT, an AI designed to converse with users, can answer various questions.'),\n",
       " Document(metadata={}, page_content='ChatGPT was developed by OpenAI and is continuously being improved.'),\n",
       " Document(metadata={}, page_content='ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.'),\n",
       " Document(metadata={}, page_content='ChatGPT can be used to solve complex problems or suggest creative ideas.'),\n",
       " Document(metadata={}, page_content=\"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\")]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What can you tell me about ChatGPT?\"\n",
    "\n",
    "# Retrieves relevant documents sorted by relevance score.\n",
    "docs = retriever.invoke(query)\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个 `LongContextReorder` 类的实例。\n",
    "\n",
    "- 调用 `reordering.transform_documents(docs)` 来重新排序文档列表。\n",
    "- 相关性较低的文档会被置于列表的中间，而相关性较高的文档会被放置在列表的开头和结尾。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={}, page_content='The FIFA World Cup is held every four years and is the biggest event in international football.'),\n",
       " Document(metadata={}, page_content='iPhone, iPad, MacBook are representative products released by Apple.'),\n",
       " Document(metadata={}, page_content='ChatGPT, an AI designed to converse with users, can answer various questions.'),\n",
       " Document(metadata={}, page_content='ChatGPT has learned from vast amounts of data to understand user questions and generate appropriate answers.'),\n",
       " Document(metadata={}, page_content=\"ChatGPT's capabilities are continuously evolving through ongoing learning and updates.\"),\n",
       " Document(metadata={}, page_content='ChatGPT can be used to solve complex problems or suggest creative ideas.'),\n",
       " Document(metadata={}, page_content='ChatGPT was developed by OpenAI and is continuously being improved.'),\n",
       " Document(metadata={}, page_content='This is just a random text I wrote.'),\n",
       " Document(metadata={}, page_content=\"Wearable devices like Apple Watch and AirPods are also part of Apple's popular product line.\"),\n",
       " Document(metadata={}, page_content='Bitcoin is also called digital gold and is gaining popularity as a store of value.')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_transformers import LongContextReorder\n",
    "reordering = LongContextReorder()\n",
    "reordered_docs = reordering.transform_documents(docs)\n",
    "\n",
    "reordered_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "源码分析\n",
    "\n",
    "```python\n",
    "documents.reverse()\n",
    "reordered_result = []\n",
    "for i, value in enumerate(documents):\n",
    "\tif i % 2 == 1:\n",
    "\t\treordered_result.append(value)\n",
    "\telse:\n",
    "\t\treordered_result.insert(0, value)\n",
    "```\n",
    "原顺序是相似度由高到低的, 他只是在原顺序的基础上把高相似度的放散在头部和尾部, 低相关的放在中部.  \n",
    "\n",
    "> 当模型需要在长上下文的中间部分访问相关信息时，它往往会忽视提供的文档\n",
    "\n",
    "按这种说法, 模型会更注重头部和尾部的文档"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
