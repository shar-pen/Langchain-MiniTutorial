{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Document**  \n",
    "\n",
    "用于存储一段文本及其相关元数据的类。  \n",
    "\n",
    "- **`page_content`** （必需）：以字符串形式存储一段文本。  \n",
    "- **`metadata`** （可选）：以字典形式存储与 `page_content` 相关的元数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={}, page_content='Hello, welcome to LangChain Open Tutorial!')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "document = Document(page_content=\"Hello, welcome to LangChain Open Tutorial!\")\n",
    "\n",
    "document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **文档加载器（Document Loader）**  \n",
    "\n",
    "**文档加载器**是一个用于从**各种来源**加载 `Document` 的类。  \n",
    "\n",
    "以下是一些常见的文档加载器示例：  \n",
    "\n",
    "- **`PyPDFLoader`** ：加载 PDF 文件  \n",
    "- **`CSVLoader`** ：加载 CSV 文件  \n",
    "- **`UnstructuredHTMLLoader`** ：加载 HTML 文件  \n",
    "- **`JSONLoader`** ：加载 JSON 文件  \n",
    "- **`TextLoader`** ：加载纯文本文件  \n",
    "- **`DirectoryLoader`** ：从目录中批量加载文档  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Set up the loader\n",
    "FILE_PATH = \"./data/01-document-loader-sample.pdf\"\n",
    "loader = PyPDFLoader(FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **load()**  \n",
    "\n",
    "- 加载文档，并以 `list[Document]` 的形式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n",
      "---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content=' \\n \\n \\nOctober 2016 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTHE NATIONAL  \\nARTIFICIAL INTELLIGENCE \\nRESEARCH AND DEVELOPMENT \\nSTRATEGIC PLAN  \\nNational Science and Technology Council \\n \\nNetworking and Information Technology \\nResearch and Development Subcommittee \\n '),\n",
       " Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 1}, page_content=' ii \\n \\n ')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = loader.load()\n",
    "print(len(docs))\n",
    "print('-'*3)\n",
    "docs[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **aload()**  \n",
    "\n",
    "- **异步**加载文档，并以 `list[Document]` 的形式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents asynchronously\n",
    "docs = await loader.aload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **lazy_load()**  \n",
    "\n",
    "- **顺序**加载文档，并以 `Iterator[Document]` 的形式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = loader.lazy_load()\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "    break  # Used to limit the output length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **alazy_load()**  \n",
    "\n",
    "- **异步**顺序加载文档，并以 `AsyncIterator[Document]` 的形式返回。\n",
    "\n",
    "可以观察到，这种方法作为一个 **`async_generator`** 工作。它是一种特殊类型的异步迭代器，能够**按需生成**值，而不需要一次性将所有值存储在内存中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader.alazy_load()\n",
    "docs = loader.alazy_load()\n",
    "async for doc in docs:\n",
    "    print(doc.metadata)\n",
    "    break  # Used to limit the output length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **load_and_split()**  \n",
    "\n",
    "- 加载文档，并使用 `TextSplitter` **自动拆分**为多个文本块，最终以 `list[Document]` 的形式返回。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1430\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content='October 2016 \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTHE NATIONAL  \\nARTIFICIAL INTELLIGENCE \\nRESEARCH AND DEVELOPMENT \\nSTRATEGIC PLAN'),\n",
       " Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 0}, page_content='National Science and Technology Council \\n \\nNetworking and Information Technology \\nResearch and Development Subcommittee'),\n",
       " Document(metadata={'source': './data/01-document-loader-sample.pdf', 'page': 1}, page_content='ii')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Set up the TextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=128, chunk_overlap=0)\n",
    "\n",
    "# Split Documents into chunks\n",
    "docs = loader.load_and_split(text_splitter=text_splitter)\n",
    "\n",
    "print(len(docs))\n",
    "docs[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PyPDF` 是最广泛使用的 Python 库之一，用于 PDF 处理。\n",
    "\n",
    "在这里，我们使用 `pypdf` 将 PDF 加载为文档数组，每个文档包含一个 `page` 页码，并包含页面内容和元数据。\n",
    "\n",
    "LangChain 的 [PyPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyPDFLoader.html) 集成了 PyPDF，将 PDF 文档解析为 LangChain 的 Document 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NATIONAL ARTIFICIAL INTELLIGENCE RESEARCH AND DEVELOPMENT STRATEGIC PLAN \n",
      " \n",
      " 3 \n",
      "Executive Summary \n",
      "Artificial intelligence (AI) is a transformative technology that holds promise for tremendous societal and \n",
      "economic benefit. AI has the potential to revolutionize how we live, work, learn, discover, a\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# Initialize the PDF loader\n",
    "FILE_PATH = \"./data/01-document-loader-sample.pdf\"\n",
    "loader = PyPDFLoader(FILE_PATH)\n",
    "\n",
    "# Load data into Document objects\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the contents of the document\n",
    "print(docs[10].page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDF(OCR)\n",
    "\n",
    "有些 PDF 包含扫描文档或图片中的文本图像。你也可以使用 `rapidocr-onnxruntime` 包从图像中提取文本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyPDFLoader(FILE_PATH, extract_images=True)\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyPDF Directory\n",
    "\n",
    "从目录中导入所有 PDF 文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "# directory path\n",
    "loader = PyPDFDirectoryLoader(\"./data/\")\n",
    "\n",
    "# load documents\n",
    "docs = loader.load()\n",
    "\n",
    "# print the number of documents\n",
    "print(len(docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PyMuPDF**  \n",
    "\n",
    "`PyMuPDF` 经过速度优化，并提供关于 PDF 及其页面的详细元数据。它**每页返回一个文档**。  \n",
    "\n",
    "LangChain 的 [PyMuPDFLoader](https://python.langchain.com/api_reference/community/document_loaders/langchain_community.document_loaders.pdf.PyMuPDFLoader.html) 集成了 `PyMuPDF`，可将 PDF 文档解析为 LangChain 的 `Document` 对象。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/document_loaders/parsers/pdf.py:322: UserWarning: Warning: Empty content on page 4 of document ./data/01-document-loader-sample.pdf\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "# create an instance of the PyMuPDF loader\n",
    "FILE_PATH = \"./data/01-document-loader-sample.pdf\"\n",
    "loader = PyMuPDFLoader(FILE_PATH)\n",
    "\n",
    "# load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# print the contents of the document\n",
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebBaseLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`WebBaseLoader` 是 LangChain 中一个专门用于处理基于网页内容的文档加载器。\n",
    "\n",
    "它利用 `BeautifulSoup4` 库有效地解析网页，并通过 `SoupStrainer` 和其他 `bs4` 参数提供可自定义的解析选项。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 2\n",
      " \n",
      "【Python】Hello World 输入输出_python你好世界输入,输出英文-CSDN博客\n",
      "【Python】Hello World 输入输出\n",
      "最新推荐文章于 2024-12-04 08:5\n"
     ]
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Load news article content using WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "   web_paths=(\n",
    "\t\"https://blog.csdn.net/wait_for_taht_day5/article/details/50570827\",\n",
    "\t\"https://blog.csdn.net/teethfairy/article/details/7287307\"\n",
    "\t),\n",
    "   encoding='utf-8'\n",
    ")\n",
    "\n",
    "# Load and process the documents\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\")\n",
    "\n",
    "import re\n",
    "print(re.sub(r'\\n+', '\\n', docs[0].page_content)[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSV Loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='PassengerId: 1\n",
      "Survived: 0\n",
      "Pclass: 3\n",
      "Name: Braund, Mr. Owen Harris\n",
      "Sex: male\n",
      "Age: 22\n",
      "SibSp: 1\n",
      "Parch: 0\n",
      "Ticket: A/5 21171\n",
      "Fare: 7.25\n",
      "Cabin: \n",
      "Embarked: S' metadata={'source': './data/titanic.csv', 'row': 0}\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders.csv_loader import CSVLoader\n",
    "\n",
    "# Create CSVLoader instance\n",
    "loader = CSVLoader(file_path=\"./data/titanic.csv\")\n",
    "\n",
    "# Load documents\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: \\nEmbarked: S'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "它会读取 header 然后把每行数据重新组织\n",
    "\n",
    "原始数据\n",
    "```\n",
    "PassengerId,Survived,Pclass,Name,Sex,Age,SibSp,Parch,Ticket,Fare,Cabin,Embarked \n",
    "1,0,3,\"Braund, Mr. Owen Harris\",male,22,1,0,A/5 21171,7.25,,S\n",
    "```\n",
    "---\n",
    "```\n",
    "PassengerId: 1\\nSurvived: 0\\nPclass: 3\\nName: Braund, Mr. Owen Harris\\nSex: male\\nAge: 22\\nSibSp: 1\\nParch: 0\\nTicket: A/5 21171\\nFare: 7.25\\nCabin: \\nEmbarked: S\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **自定义 CSV 解析和加载**\n",
    "\n",
    "`CSVLoader` 接受一个 `csv_args` 关键字参数，用于定制传递给 Python 的 `csv.DictReader` 的参数。这使得你可以处理各种 CSV 格式，如自定义分隔符、引号字符或特定的换行符处理。\n",
    "\n",
    "有关支持的 `csv_args` 及如何根据你的特定需求定制解析的更多信息，请参见 Python 的 [csv 模块](https://docs.python.org/3/library/csv.html) 文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger ID: 1\n",
      "Survival (1: Survived, 0: Died): 0\n",
      "Passenger Class: 3\n",
      "Name: Braund, Mr. Owen Harris\n",
      "Sex: male\n",
      "Age: 22\n",
      "Number of Siblings/Spouses Aboard: 1\n",
      "Number of Parents/Children Aboard: 0\n",
      "Ticket Number: A/5 21171\n",
      "Fare: 7.25\n",
      "Cabin: \n",
      "Port of Embarkation: S\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"./data/titanic.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\n",
    "            \"Passenger ID\",\n",
    "            \"Survival (1: Survived, 0: Died)\",\n",
    "            \"Passenger Class\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Age\",\n",
    "            \"Number of Siblings/Spouses Aboard\",\n",
    "            \"Number of Parents/Children Aboard\",\n",
    "            \"Ticket Number\",\n",
    "            \"Fare\",\n",
    "            \"Cabin\",\n",
    "            \"Port of Embarkation\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passenger ID: 1\n",
      "Survival (1: Survived, 0: Died): 0\n",
      "Passenger Class: 3\n",
      "Name: Braund, Mr. Owen Harris\n",
      "Sex: male\n",
      "Age: 22\n",
      "None: 1,0,A/5 21171,7.25,,S\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"./data/titanic.csv\",\n",
    "    csv_args={\n",
    "        \"delimiter\": \",\",\n",
    "        \"quotechar\": '\"',\n",
    "        \"fieldnames\": [\n",
    "            \"Passenger ID\",\n",
    "            \"Survival (1: Survived, 0: Died)\",\n",
    "            \"Passenger Class\",\n",
    "            \"Name\",\n",
    "            \"Sex\",\n",
    "            \"Age\",\n",
    "        ],\n",
    "    },\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[1].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些参数其实没啥好解释的, fieldnames 不是用于选择列的, 而是重新命名的, 如果部分列没命名, 那会分到 None 列"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你应该使用 `source_column` 参数来指定每一行生成文档的来源。否则，`file_path` 将作为所有从 CSV 文件创建的文档的来源。\n",
    "\n",
    "当在一个用于基于源信息回答问题的链中使用从 CSV 文件加载的文档时，这一点尤其有用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='PassengerId: 2\n",
      "Survived: 1\n",
      "Pclass: 1\n",
      "Name: Cumings, Mrs. John Bradley (Florence Briggs Thayer)\n",
      "Sex: female\n",
      "Age: 38\n",
      "SibSp: 1\n",
      "Parch: 0\n",
      "Ticket: PC 17599\n",
      "Fare: 71.2833\n",
      "Cabin: C85\n",
      "Embarked: C' metadata={'source': '2', 'row': 1}\n"
     ]
    }
   ],
   "source": [
    "loader = CSVLoader(\n",
    "    file_path=\"./data/titanic.csv\",\n",
    "    source_column=\"PassengerId\",  # Specify the source column\n",
    ")\n",
    "\n",
    "docs = loader.load()  \n",
    "\n",
    "print(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意 metadata.source 变成了对应的 PassengerId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DataFrameLoader**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`Pandas` 是一个开源的数据分析和处理工具，专为 Python 编程语言设计。该库在数据科学、机器学习以及多个领域中广泛应用，用于处理各种数据。\n",
    "\n",
    "LangChain 的 `DataFrameLoader` 是一个强大的工具，旨在无缝地将 `Pandas` `DataFrame` 集成到 LangChain 工作流中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   PassengerId  Survived  Pclass  \\\n",
      "0            1         0       3   \n",
      "1            2         1       1   \n",
      "2            3         1       3   \n",
      "3            4         1       1   \n",
      "4            5         0       3   \n",
      "\n",
      "                                                Name     Sex   Age  SibSp  \\\n",
      "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
      "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
      "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
      "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
      "4                           Allen, Mr. William Henry    male  35.0      0   \n",
      "\n",
      "   Parch            Ticket     Fare Cabin Embarked  \n",
      "0      0         A/5 21171   7.2500   NaN        S  \n",
      "1      0          PC 17599  71.2833   C85        C  \n",
      "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
      "3      0            113803  53.1000  C123        S  \n",
      "4      0            373450   8.0500   NaN        S  \n",
      "Braund, Mr. Owen Harris\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./data/titanic.csv\")\n",
    "# df = pd.read_excel(\"./data/titanic.xlsx\")\n",
    "\n",
    "print(df.head(n=5))\n",
    "# The Name column of the DataFrame is specified to be used as the content of each document.\n",
    "loader = DataFrameLoader(df, page_content_column=\"Name\")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(docs[0].page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可通过 page_content_column 来指定哪些 dataframe 的列被读取. csv 和 excel 文件都可以用 DataFrameLoader 加载"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Docx2txtLoader**  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 采用轻量级的 Python 模块 **`docx2txt`** 进行文本提取。  \n",
    "- **快速**、**简单**地从 `.docx` 文件中提取文本。  \n",
    "- 适用于**高效且直接**的文本处理任务。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Metadata: {'source': 'data/sample-word-document_eng.docx'}\n",
      "\n",
      "-----\n",
      "Semantic Search\n",
      "\n",
      "\n",
      "\n",
      "Definition: Semantic search is a search methodology that goes beyond simple keywo\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "\n",
    "# Initialize the document loader\n",
    "loader = Docx2txtLoader(\"data/sample-word-document_eng.docx\")\n",
    "\n",
    "# Load the document\n",
    "docs = loader.load()\n",
    "\n",
    "# Print the metadata of the document\n",
    "print(f\"Document Metadata: {docs[0].metadata}\\n\")\n",
    "\n",
    "# Note: The entire docx file is converted into a single document.\n",
    "# It needs to be split into smaller parts using a text splitter.\n",
    "print('-'*5)\n",
    "print(docs[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不像 PDF Loader 会默认根据页切分文档, Docx2txtLoader 会把一个文件读取为一个 Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TXT Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "\n",
      "[Metadata]\n",
      "\n",
      "{'source': 'data/appendix-keywords.txt'}\n",
      "\n",
      "========= [Preview - First 500 Characters] =========\n",
      "\n",
      "Semantic Search\n",
      "\n",
      "Definition: Semantic search is a search method that goes beyond simple keyword matching by understanding the meaning of the user’s query to return relevant results.\n",
      "Example: If a user searches for “planets in the solar system,” the system might return information about related planets such as “Jupiter” or “Mars.”\n",
      "Related Keywords: Natural Language Processing, Search Algorithms, Data Mining\n",
      "\n",
      "Embedding\n",
      "\n",
      "Definition: Embedding is the process of converting textual data, such as words\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# Create a text loader\n",
    "loader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "\n",
    "# Load the document\n",
    "docs = loader.load()\n",
    "print(f\"Number of documents: {len(docs)}\\n\")\n",
    "print(\"[Metadata]\\n\")\n",
    "print(docs[0].metadata)\n",
    "print(\"\\n========= [Preview - First 500 Characters] =========\\n\")\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JSONLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 1}, page_content='{\"name\": {\"first\": \"Alice\", \"last\": \"Johnson\"}, \"age\": 28, \"contact\": {\"email\": \"alice.johnson@example.com\", \"phone\": \"+1-555-0123\", \"social_media\": {\"twitter\": \"@alice_j\", \"linkedin\": \"linkedin.com/in/alicejohnson\"}}, \"address\": {\"street\": \"123 Maple St\", \"city\": \"Springfield\", \"state\": \"IL\", \"zip\": \"62704\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Reading\", \"Hiking\", \"Cooking\"], \"favorite_food\": \"Italian\", \"music_genre\": \"Jazz\", \"travel_destinations\": [\"Japan\", \"Italy\", \"Canada\"]}, \"interesting_fact\": \"Alice has traveled to over 15 countries and speaks 3 languages.\"}'),\n",
       " Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 2}, page_content='{\"name\": {\"first\": \"Bob\", \"last\": \"Smith\"}, \"age\": 34, \"contact\": {\"email\": \"bob.smith@example.com\", \"phone\": \"+1-555-0456\", \"social_media\": {\"twitter\": \"@bobsmith34\", \"linkedin\": \"linkedin.com/in/bobsmith\"}}, \"address\": {\"street\": \"456 Oak Ave\", \"city\": \"Metropolis\", \"state\": \"NY\", \"zip\": \"10001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Photography\", \"Cycling\", \"Video Games\"], \"favorite_food\": \"Mexican\", \"music_genre\": \"Rock\", \"travel_destinations\": [\"Brazil\", \"Australia\", \"Germany\"]}, \"interesting_fact\": \"Bob is an avid gamer and has competed in several national tournaments.\"}'),\n",
       " Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 3}, page_content='{\"name\": {\"first\": \"Charlie\", \"last\": \"Davis\"}, \"age\": 45, \"contact\": {\"email\": \"charlie.davis@example.com\", \"phone\": \"+1-555-0789\", \"social_media\": {\"twitter\": \"@charliedavis45\", \"linkedin\": \"linkedin.com/in/charliedavis\"}}, \"address\": {\"street\": \"789 Pine Rd\", \"city\": \"Gotham\", \"state\": \"NJ\", \"zip\": \"07001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Gardening\", \"Fishing\", \"Woodworking\"], \"favorite_food\": \"Barbecue\", \"music_genre\": \"Country\", \"travel_destinations\": [\"Canada\", \"New Zealand\", \"Norway\"]}, \"interesting_fact\": \"Charlie has a small farm where he raises chickens and grows organic vegetables.\"}'),\n",
       " Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 4}, page_content='{\"name\": {\"first\": \"Dana\", \"last\": \"Lee\"}, \"age\": 22, \"contact\": {\"email\": \"dana.lee@example.com\", \"phone\": \"+1-555-0111\", \"social_media\": {\"twitter\": \"@danalee22\", \"linkedin\": \"linkedin.com/in/danalee\"}}, \"address\": {\"street\": \"234 Birch Blvd\", \"city\": \"Star City\", \"state\": \"CA\", \"zip\": \"90001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Dancing\", \"Sketching\", \"Traveling\"], \"favorite_food\": \"Thai\", \"music_genre\": \"Pop\", \"travel_destinations\": [\"Thailand\", \"France\", \"Spain\"]}, \"interesting_fact\": \"Dana is a dance instructor and has won several local competitions.\"}'),\n",
       " Document(metadata={'source': '/data02/hyzhang10/pengxia2/LangChain-tutorial/data/people.json', 'seq_num': 5}, page_content='{\"name\": {\"first\": \"Ethan\", \"last\": \"Garcia\"}, \"age\": 31, \"contact\": {\"email\": \"ethan.garcia@example.com\", \"phone\": \"+1-555-0999\", \"social_media\": {\"twitter\": \"@ethangarcia31\", \"linkedin\": \"linkedin.com/in/ethangarcia\"}}, \"address\": {\"street\": \"345 Cedar St\", \"city\": \"Central City\", \"state\": \"TX\", \"zip\": \"75001\", \"country\": \"USA\"}, \"personal_preferences\": {\"hobbies\": [\"Running\", \"Travel Blogging\", \"Cooking\"], \"favorite_food\": \"Indian\", \"music_genre\": \"Hip-Hop\", \"travel_destinations\": [\"India\", \"Italy\", \"Mexico\"]}, \"interesting_fact\": \"Ethan runs a popular travel blog where he shares his adventures and culinary experiences.\"}')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "\n",
    "# Create JSONLoader\n",
    "loader = JSONLoader(\n",
    "    file_path=\"data/people.json\",\n",
    "    jq_schema=\".people[]\",  # Access each item in the people array\n",
    "    text_content=False,\n",
    ")\n",
    "\n",
    "# Example: extract only contact_details\n",
    "# loader = JSONLoader(\n",
    "#     file_path=\"data/people.json\",\n",
    "#     jq_schema=\".people[].contact_details\",\n",
    "#     text_content=False,\n",
    "# )\n",
    "\n",
    "# Or extract only hobbies from personal_preferences\n",
    "# loader = JSONLoader(\n",
    "#     file_path=\"data/people.json\",\n",
    "#     jq_schema=\".people[].personal_preferences.hobbies\",\n",
    "#     text_content=False,\n",
    "# )\n",
    "\n",
    "# Load documents\n",
    "docs = loader.load()\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
