{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 向量存储的概念指南\n",
    "\n",
    "**向量存储**是专门设计的数据库，用于通过向量表示（嵌入）来**索引**和**检索**信息。\n",
    "\n",
    "它们通常用于通过识别语义相似的内容而非依赖精确的关键词匹配，来搜索非结构化数据，如文本、图像和音频。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么向量存储至关重要\n",
    "\n",
    "1. **快速高效的搜索**\n",
    "\n",
    "通过正确存储和索引嵌入向量，向量存储能够快速检索相关信息，即使在处理海量数据集时也能高效工作。\n",
    "\n",
    "2. **应对数据增长的可扩展性**\n",
    "\n",
    "随着数据不断扩展，向量存储必须能够高效扩展。一个结构良好的向量存储可以确保系统处理大规模数据时不会出现性能问题，支持无缝增长。\n",
    "\n",
    "3. **促进语义搜索**\n",
    "\n",
    "与传统的基于关键词的搜索不同，语义搜索是根据内容的含义来检索信息。向量存储通过查找与用户查询上下文密切相关的段落或部分来实现这一点。这相比于仅能进行精确关键词匹配的原始文本数据库具有明显优势。\n",
    "\n",
    "\n",
    "## 理解搜索方法\n",
    "\n",
    "- **基于关键词的搜索**  \n",
    "  这种方法依赖于通过查询与文档中精确的单词或短语匹配来检索结果。它简单，但无法捕捉单词之间的语义关系。\n",
    "\n",
    "- **基于相似度的搜索**  \n",
    "  使用向量表示来评估查询与文档之间的语义相似度。它提供了更准确的结果，尤其是对于自然语言查询。\n",
    "\n",
    "- **基于分数的相似度搜索**  \n",
    "  根据查询为每个文档分配一个相似度分数。更高的分数表示相关性更强。常用的度量标准包括 `余弦相似度` 或 `基于距离的评分`。\n",
    "\n",
    "## 相似度搜索的工作原理\n",
    "\n",
    "- **嵌入和向量的概念**  \n",
    "  `嵌入` 是单词或文档在高维空间中的数值表示。它们捕捉了语义信息，从而能够更好地比较查询和文档。\n",
    "\n",
    "- **相似度度量方法**  \n",
    "  - **余弦相似度**：衡量两个向量之间的夹角的余弦值。值越接近 1 表示相似度越高。  \n",
    "  - **欧几里得距离**：计算向量空间中两个点之间的直线距离。距离越小表示相似度越高。\n",
    "\n",
    "- **评分和排序搜索结果**  \n",
    "  计算相似度后，文档将被分配一个分数。根据这些分数，结果将按相关性降序排列。\n",
    "\n",
    "- **搜索算法简要概述**  \n",
    "  - `TF-IDF`：根据单词在文档中的频率及其在所有文档中的出现频率来赋予单词权重。  \n",
    "  - `BM25`：`TF-IDF` 的改进版，优化了信息检索的相关性。  \n",
    "  - `神经搜索`：利用深度学习生成上下文感知的嵌入，以获得更准确的结果。\n",
    "\n",
    "## 向量存储中的搜索类型\n",
    "\n",
    "- **相似度搜索**：找到与查询最相似的文档。适用于语义搜索应用。\n",
    "\n",
    "- **最大边际相关性 (MMR) 搜索**：通过优先选择多样且相关的文档，在搜索结果中平衡相关性和多样性。\n",
    "\n",
    "- **稀疏检索器**：使用传统的基于关键词的方法，如 `TF-IDF` 或 `BM25` 来检索文档。适用于上下文有限的数据集。\n",
    "\n",
    "- **密集检索器**：依赖密集的向量嵌入来捕捉语义信息。常见于使用深度学习的现代搜索系统。\n",
    "\n",
    "- **混合搜索**：结合稀疏和密集的检索方法。通过将密集方法的精确度与稀疏方法的广泛覆盖性结合，达到最优结果。\n",
    "\n",
    "## 向量存储作为检索器\n",
    "- **功能**：通过使用 `.as_retriever()` 方法将向量存储转换为检索器，你可以创建一个符合 LangChain 检索器接口的轻量级包装器。这使得你可以使用不同的检索策略，如 `相似度搜索` 和 `最大边际相关性 (MMR) 搜索`，并允许自定义检索参数。  \n",
    "- **使用案例**：适用于复杂的应用场景，在这些场景中，检索器需要作为更大流程的一部分，例如检索增强生成 (RAG) 系统。它便于与 LangChain 中的其他组件无缝集成，支持诸如集成检索方法和高级查询分析等功能。\n",
    "\n",
    "总之，虽然直接的向量存储搜索提供了基本的检索能力，但将向量存储转换为检索器，可以提供更强的灵活性和在 LangChain 生态系统中的集成，支持更复杂的检索策略和应用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 集成向量数据库\n",
    "\n",
    "\n",
    "- **Chroma**：一个开源的向量数据库，专为 AI 应用设计，支持高效存储和检索嵌入向量。\n",
    "- **FAISS**：由 Facebook AI 开发，FAISS（Facebook AI Similarity Search）是一个用于高效相似度搜索和密集向量聚类的库。\n",
    "- **Pinecone**：一个托管的向量数据库服务，提供高性能的向量相似度搜索，使开发人员能够构建可扩展的 AI 应用程序。\n",
    "- **Qdrant**：Qdrant（读作 quadrant）是一个向量相似度搜索引擎。它提供了一个生产就绪的服务，具有便捷的 API，用于存储、搜索和管理向量，支持额外的有效载荷和扩展过滤功能。它适用于各种神经网络或基于语义的匹配、分面搜索和其他应用。\n",
    "- **Elasticsearch**：一个分布式的 RESTful 搜索和分析引擎，支持向量搜索，允许在大型数据集中高效进行相似度搜索。\n",
    "- **MongoDB**：MongoDB Atlas 向量搜索支持向量嵌入的高效存储、索引和查询，并与操作数据一起使用，方便无缝实现 AI 驱动的应用。\n",
    "- **pgvector (PostgreSQL)**：一个 PostgreSQL 扩展，添加了向量相似度搜索功能，使得在关系数据库中高效存储和查询向量数据成为可能。\n",
    "- **Neo4j**：一个图形数据库，存储节点和关系，并原生支持向量搜索，方便执行涉及图形和向量数据的复杂查询。\n",
    "- **Weaviate**：一个开源的向量数据库，允许存储数据对象和向量嵌入，支持多种数据类型并提供语义搜索功能。\n",
    "- **Milvus**：一个数据库，专为存储、索引和管理机器学习模型生成的大规模嵌入向量而设计，支持高性能的向量相似度搜索。\n",
    "\n",
    "这些向量存储在构建需要高效相似度搜索和高维数据管理的应用程序中起着至关重要的作用。\n",
    "\n",
    "LangChain 提供了一个统一的接口，用于与向量存储交互，使用户可以轻松切换不同的实现方式。\n",
    "\n",
    "该接口包括用于 **写入**、**删除** 和 **搜索** 向量存储中的文档的核心方法。\n",
    "\n",
    "主要方法如下：\n",
    "\n",
    "- `add_documents` : 将一组文本添加到向量存储中。\n",
    "- `upsert_documents` : 向向量存储中添加新文档，或者如果文档已存在，则更新它们。\n",
    "  - 在本教程中，我们还将介绍 `upsert_documents_parallel` 方法，它在适用时支持高效的大规模数据处理。\n",
    "- `delete_documents` : 从向量存储中删除一组文档。\n",
    "- `similarity_search` : 搜索与给定查询相似的文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chorma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "vector_store = Chroma(\n",
    "\tcollection_name=\"langchain_store\", \n",
    "\tembedding_function=embeddings\n",
    "\t)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Documents\n",
    "\n",
    "将一组 Documents 添加到向量存储中, 并生成一组 id, id 可以自己预设置  \n",
    "返回每个 Documents 的 id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果 embedding 功能会出现 InternalServerError, 可能文档文本有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果不指定 ID，Chroma 会自动为文档生成唯一 ID，但手动指定 ID 可以帮助你更方便地管理文档。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\n",
    "    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",\n",
    "    \"AI can analyze medical images to detect conditions like cancer.\",\n",
    "    \"Machine learning predicts patient outcomes based on health data.\",\n",
    "    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",\n",
    "    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",\n",
    "    \"AI automates administrative tasks, saving time for healthcare workers.\",\n",
    "    \"NLP extracts insights from electronic health records for better care.\",\n",
    "    \"AI chatbots help with patient assessments and symptom checking.\",\n",
    "    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",\n",
    "    \"AI optimizes hospital operations and reduces healthcare costs.\"\n",
    "]\n",
    "# 重复内容会导致报错\n",
    "# texts = [\n",
    "#     \"AI \",\n",
    "#     \"AI\",\n",
    "#     \"Machine learning\",\n",
    "# ]\n",
    "documents = [\n",
    "\tDocument(text, metadata={\"source\":text})\n",
    "\tfor text in texts\n",
    "]\n",
    "ids = [f'{i}' for i in range(len(documents))]\n",
    "\n",
    "ret_ids = vector_store.add_documents(documents=documents, ids=ids)\n",
    "\n",
    "print(ret_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重复添加对于不同向量数据集的处理方式不一样。  \n",
    "重复添加在 chrome 中不会报错, 应该是直接替代了 id 的原数据, 但换做 Faiss 就会报错, 报错信息显示 id 已存在。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ids = vector_store.add_documents(documents=documents, ids=ids)\n",
    "\n",
    "print(ret_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_ids = vector_store.add_documents(documents=documents)\n",
    "\n",
    "print(ret_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Chroma' object has no attribute 'upsert_documents'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m vector_store\u001b[38;5;241m.\u001b[39madd_documents(documents\u001b[38;5;241m=\u001b[39m[doc1], ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 使用 upsert_documents 更新或插入文档\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m \u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert_documents\u001b[49m(documents\u001b[38;5;241m=\u001b[39m[doc2], ids\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoc_1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Chroma' object has no attribute 'upsert_documents'"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "doc1 = Document(page_content=\"This is a test document.\", metadata={\"source\": \"test\"})\n",
    "doc2 = Document(page_content=\"This is an updated document.\", metadata={\"source\": \"test\"})\n",
    "\n",
    "# 文档 ID 可以根据文本的哈希值来生成唯一的 ID\n",
    "ids = [\"doc_1\", \"doc_2\"]\n",
    "\n",
    "# 第一次插入\n",
    "vector_store.add_documents(documents=[doc1], ids=[\"doc_1\"])\n",
    "\n",
    "# 使用 upsert_documents 更新或插入文档\n",
    "vector_store.upsert_documents(documents=[doc2], ids=[\"doc_1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chroma 没有 upsert_documents 方法, 但有同功能的函数 update_documents, 貌似 add_documents 起到类似的功能(对于一个 id, 更新为新的 Document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "ret_ids = vector_store.update_documents(ids=ids, documents=documents)\n",
    "\n",
    "print(ret_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete Documents by id\n",
    "\n",
    "chrome 没有 delete_documents 函数, 只能基于 id 删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_4_delete = [\"3\"]\n",
    "\n",
    "import traceback\n",
    "try:\n",
    "\tvector_store.delete(ids=ids_4_delete)\n",
    "\tprint(f'ID:{ids_4_delete} deleted')\n",
    "except Exception as e:\n",
    "\tprint(traceback.format_exc())\n",
    "\tprint('deleted non-successfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "删除整个数据库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store.delete_collection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search\n",
    "\n",
    "- `similarity_search` 只返回 Documents\n",
    "- `similarity_search_with_score` 返回 Documents 和对应的 scores\n",
    "- `similarity_search_by_vector` 可自己转入 embedding vector, 因此可以自己控制 query 的 embedding 与 vecotr database 的 embedding 方法不同. 该函数也有带 scores 返回的版本\n",
    "\n",
    "similarity_search 方法会基于给定的查询和嵌入向量，在向量存储中查找与之最相似的文档。它通常用于简单的、单次的检索操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]AI monitors patients remotely, enabling proactive care for chronic diseases. [{'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}]\n",
      "[1]AI chatbots help with patient assessments and symptom checking. [{'source': 'AI chatbots help with patient assessments and symptom checking.'}]\n",
      "[2]AI automates administrative tasks, saving time for healthcare workers. [{'source': 'AI automates administrative tasks, saving time for healthcare workers.'}]\n"
     ]
    }
   ],
   "source": [
    "a_query = \"How does AI improve healthcare?\"\n",
    "results = vector_store.similarity_search(\n",
    "\tquery=a_query,\n",
    "\tk=3\n",
    "\t)\n",
    "for index, doc in enumerate(results):\n",
    "    print(f\"[{index}]{doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0][SIM=0.718768]AI monitors patients remotely, enabling proactive care for chronic diseases. [{'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}]\n",
      "[1][SIM=0.807140]AI chatbots help with patient assessments and symptom checking. [{'source': 'AI chatbots help with patient assessments and symptom checking.'}]\n",
      "[2][SIM=0.815210]AI automates administrative tasks, saving time for healthcare workers. [{'source': 'AI automates administrative tasks, saving time for healthcare workers.'}]\n"
     ]
    }
   ],
   "source": [
    "a_query = \"How does AI improve healthcare?\"\n",
    "results = vector_store.similarity_search_with_score(\n",
    "\tquery=a_query,\n",
    "\tk=3\n",
    "\t)\n",
    "\n",
    "for index, (doc, score) in enumerate(results):\n",
    "    print(f\"[{index}][SIM={score:3f}]{doc.page_content} [{doc.metadata}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedder = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "query_vector = query_embedder.embed_query(a_query)\n",
    "results = db.similarity_search_by_vector(query_vector, k=3)\n",
    "for index, doc in enumerate(results):\n",
    "    print(f\"[{index}]{doc.page_content} [{doc.metadata}]\")\n",
    "\n",
    "\"\"\" 带 score 版本\n",
    "results = db.similarity_search_with_relevance_scores(query_vector, k=3)\n",
    "for index, (doc, score) in enumerate(results):\n",
    "    print(f\"[{index}][SIM={score:3f}]{doc.page_content} [{doc.metadata}]\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## as_retriever \n",
    "\n",
    "as_retriever 方法是 Chroma 向量数据库的一个重要功能，它将 向量数据库 转换为一个 Retriever 对象，这样你就可以在 LangChain 的检索管道中使用它。as_retriever 使得 Chroma 与 LangChain 更好地集成，支持不同的检索策略，并能与其他组件（如问答系统、文档检索系统等）无缝协作。\n",
    "\n",
    "以下是参数说明\n",
    "```python\n",
    "retriever = vector_store.as_retriever(\n",
    "\t# Defines the type of search that the Retriever should perform. Can be “similarity” (default), “mmr”, or “similarity_score_threshold”.\n",
    "    search_type=\"mmr\",\n",
    "\t# k: num of documents returned\n",
    "\t# fetch_k: Amount of documents to pass to MMR algorithm\n",
    "\t# lambda_mult: Diversity of results returned by MMR; 1 for minimum diversity and 0 for maximum.\n",
    "    search_kwargs={\n",
    "\t\t\"k\": 1, \n",
    "\t\t\"fetch_k\": 2, \n",
    "\t\t\"lambda_mult\": 0.5\n",
    "\t\t},\n",
    ")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x7fe87b72e3f0>, search_kwargs={'filter': {'paper_title': 'GPT-4 Technical Report'}})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve more documents with higher diversity\n",
    "# Useful if your dataset has many similar documents\n",
    "vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 6, 'lambda_mult': 0.25}\n",
    ")\n",
    "\n",
    "# Fetch more documents for the MMR algorithm to consider\n",
    "# But only return the top 5\n",
    "vector_store.as_retriever(\n",
    "    search_type=\"mmr\",\n",
    "    search_kwargs={'k': 5, 'fetch_k': 50}\n",
    ")\n",
    "\n",
    "# Only retrieve documents that have a relevance score\n",
    "# Above a certain threshold\n",
    "vector_store.as_retriever(\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    search_kwargs={'score_threshold': 0.8}\n",
    ")\n",
    "\n",
    "# Only get the single most similar document from the dataset\n",
    "vector_store.as_retriever(search_kwargs={'k': 1})\n",
    "\n",
    "# Use a filter to only retrieve documents from a specific paper\n",
    "vector_store.as_retriever(\n",
    "    search_kwargs={'filter': {'paper_title':'GPT-4 Technical Report'}}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最重要的是集成到 langchain 框架, 直接 invoke 调用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'AI monitors patients remotely, enabling proactive care for chronic diseases.'}, page_content='AI monitors patients remotely, enabling proactive care for chronic diseases.'),\n",
       " Document(metadata={'source': 'AI chatbots help with patient assessments and symptom checking.'}, page_content='AI chatbots help with patient assessments and symptom checking.'),\n",
       " Document(metadata={'source': 'AI automates administrative tasks, saving time for healthcare workers.'}, page_content='AI automates administrative tasks, saving time for healthcare workers.')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever = vector_store.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "a_query = \"How does AI improve healthcare?\"\n",
    "retriever.invoke(a_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faiss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "openai_embedding = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "embed_dim = len(openai_embedding.embed_query(\"hello world\"))\n",
    "\n",
    "index = faiss.IndexFlatL2(embed_dim)\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=openai_embedding,\n",
    "    index=index,\n",
    "    docstore= InMemoryDocstore(),\n",
    "    index_to_docstore_id={}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "texts = [\n",
    "    \"AI helps doctors diagnose diseases faster, improving patient outcomes.\",\n",
    "    \"AI can analyze medical images to detect conditions like cancer.\",\n",
    "    \"Machine learning predicts patient outcomes based on health data.\",\n",
    "    \"AI speeds up drug discovery by predicting the effectiveness of compounds.\",\n",
    "    \"AI monitors patients remotely, enabling proactive care for chronic diseases.\",\n",
    "    \"AI automates administrative tasks, saving time for healthcare workers.\",\n",
    "    \"NLP extracts insights from electronic health records for better care.\",\n",
    "    \"AI chatbots help with patient assessments and symptom checking.\",\n",
    "    \"AI improves drug manufacturing, ensuring better quality and efficiency.\",\n",
    "    \"AI optimizes hospital operations and reduces healthcare costs.\"\n",
    "]\n",
    "documents = [\n",
    "\tDocument(text)\n",
    "\tfor text in texts\n",
    "]\n",
    "ids = [f'{i}' for i in range(len(documents))]\n",
    "\n",
    "ret_ids = vector_store.add_documents(documents=documents, ids=ids)\n",
    "print(ret_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增加重复 Document 会报错\n",
    "```python\n",
    "ValueError: Tried to add ids that already exist: {'9', '5', '7', '0', '6', '8', '1', '4', '2', '3'}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_177930/1610519831.py\", line 3, in <module>\n",
      "    ret_ids = vector_store.add_documents(documents=documents, ids=ids)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_core/vectorstores/base.py\", line 286, in add_documents\n",
      "    return self.add_texts(texts, metadatas, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\", line 341, in add_texts\n",
      "    return self.__add(texts, embeddings, metadatas=metadatas, ids=ids)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/vectorstores/faiss.py\", line 316, in __add\n",
      "    self.docstore.add({id_: doc for id_, doc in zip(ids, documents)})\n",
      "  File \"/data02/hyzhang10/miniconda3/envs/xp-nlp/lib/python3.12/site-packages/langchain_community/docstore/in_memory.py\", line 28, in add\n",
      "    raise ValueError(f\"Tried to add ids that already exist: {overlapping}\")\n",
      "ValueError: Tried to add ids that already exist: {'9', '5', '7', '0', '6', '8', '1', '4', '2', '3'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import traceback\n",
    "try:\n",
    "\tret_ids = vector_store.add_documents(documents=documents, ids=ids)\n",
    "\tprint(ret_ids)\n",
    "except Exception as e:\n",
    "\tprint(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get by id\n",
    "\n",
    "通过 id 返回 Documents. Chroma 没有这个函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1', metadata={}, page_content='AI can analyze medical images to detect conditions like cancer.'),\n",
       " Document(id='2', metadata={}, page_content='Machine learning predicts patient outcomes based on health data.')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = vector_store.get_by_ids([\"1\",\"2\"])\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其他功能和 chroma 类似, 不赘述了"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
