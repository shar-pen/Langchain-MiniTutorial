# OpenAI Embeddings

æœ¬æ•™ç¨‹æ¢è®¨äº†åœ¨ `LangChain` æ¡†æ¶ä¸­ä½¿ç”¨ `OpenAI æ–‡æœ¬åµŒå…¥` æ¨¡å‹ã€‚

å±•ç¤ºå¦‚ä½•ä¸ºæ–‡æœ¬æŸ¥è¯¢å’Œæ–‡æ¡£ç”ŸæˆåµŒå…¥ï¼Œä½¿ç”¨ **PCA** é™ç»´ï¼Œå¹¶å°†å…¶å¯è§†åŒ–ä¸º 2D å›¾å½¢ä»¥ä¾¿æ›´å¥½åœ°ç†è§£ã€‚

é€šè¿‡åˆ†ææŸ¥è¯¢ä¸æ–‡æ¡£ä¹‹é—´çš„ `ä½™å¼¦ç›¸ä¼¼åº¦`ï¼Œè¯¥æ•™ç¨‹æä¾›äº†åµŒå…¥å¦‚ä½•å¢å¼ºå·¥ä½œæµçš„æ´å¯Ÿï¼ŒåŒ…æ‹¬ **æ–‡æœ¬åˆ†æ** å’Œ **æ•°æ®å¯è§†åŒ–**ã€‚


```python
from langchain_openai import OpenAIEmbeddings

# Set desired model
openai_embedding = OpenAIEmbeddings(
	model="bge-m3",
	base_url='http://localhost:9997/v1',
	api_key='cannot be empty',
	# dimensions=1024,
)

query_vector = openai_embedding.embed_query("What is the Open AI's gpt embedding model?")
```


```python
query = "How does AI improve healthcare?"

# Various embedding models
documents = [
    "AI helps doctors diagnose diseases faster, improving patient outcomes.",
    "AI can analyze medical images to detect conditions like cancer.",
    "Machine learning predicts patient outcomes based on health data.",
    "AI speeds up drug discovery by predicting the effectiveness of compounds.",
    "AI monitors patients remotely, enabling proactive care for chronic diseases.",
    "AI automates administrative tasks, saving time for healthcare workers.",
    "NLP extracts insights from electronic health records for better care.",
    "AI chatbots help with patient assessments and symptom checking.",
    "AI improves drug manufacturing, ensuring better quality and efficiency.",
    "AI optimizes hospital operations and reduces healthcare costs."
]
```

## embed æ–‡æœ¬


```python
query_vector = openai_embedding.embed_query(query)
docs_vector = openai_embedding.embed_documents(documents)

print("number of documents: " + str(len(docs_vector)))
print("dimension: " + str(len(docs_vector[0])))

# Part of the sliced â€‹â€‹vector
print("query: " + str(query_vector[:5]))
print("documents[0]: " + str(docs_vector[0][:5]))
print("documents[1]: " + str(docs_vector[1][:5]))
```

    number of documents: 10
    dimension: 1024
    query: [-0.008217299357056618, 0.05857884883880615, -0.015418872237205505, 0.02231411822140217, -0.028306419029831886]
    documents[0]: [-0.04019297659397125, 0.020847423002123833, -0.019935578107833862, -0.03813512995839119, -0.04728339985013008]
    documents[1]: [0.03994722664356232, 0.029537664726376534, -0.027887631207704544, -0.025838112458586693, -0.05827672779560089]


`embed_query` å’Œ `embed_documents` æ²¡æœ‰æ ¹æœ¬çš„ä¸åŒ, å‰è€…åªæ˜¯åè€…çš„ç‰¹æ®Šæƒ…å†µ

```python
    def embed_query(self, text: str) -> List[float]:
        """Call out to OpenAI's embedding endpoint for embedding query text.

        Args:
            text: The text to embed.

        Returns:
            Embedding for the text.
        """
        return self.embed_documents([text])[0]
```

## ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰

è¿™æ®µä»£ç é€šè¿‡**ä½™å¼¦ç›¸ä¼¼åº¦**è®¡ç®—æŸ¥è¯¢ä¸æ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚  
æ‰¾å‡ºä¸æŸ¥è¯¢æœ€ç›¸ä¼¼çš„**å‰ 3 ä¸ªæ–‡æ¡£**å’Œ**æœ€ä¸ç›¸ä¼¼çš„ 3 ä¸ªæ–‡æ¡£**ã€‚


```python
from sklearn.metrics.pairwise import cosine_similarity

# Calculate Cosine Similarity
similarity = cosine_similarity([query_vector], docs_vector)

# Sorting by in descending order
sorted_idx = similarity.argsort()[0][::-1]

# Display top 3 and bottom 3 documents based on similarity
print("query: ", query)
print("Top 3 most similar document:")
for i in range(0, 3):
    print(
        f"[{i+1}] similarity: {similarity[0][sorted_idx[i]]:.3f} | {documents[sorted_idx[i]]}"
    )

print("\nBottom 3 least similar documents:")
for i in range(1, 4):
    print(
        f"[{i}] similarity: {similarity[0][sorted_idx[-i]]:.3f} | {documents[sorted_idx[-i]]}"
    )
```

    query:  How does AI improve healthcare?
    Top 3 most similar document:
    [1] similarity: 0.641 | AI monitors patients remotely, enabling proactive care for chronic diseases.
    [2] similarity: 0.596 | AI chatbots help with patient assessments and symptom checking.
    [3] similarity: 0.592 | AI automates administrative tasks, saving time for healthcare workers.
    
    Bottom 3 least similar documents:
    [1] similarity: 0.432 | Machine learning predicts patient outcomes based on health data.
    [2] similarity: 0.485 | AI optimizes hospital operations and reduces healthcare costs.
    [3] similarity: 0.503 | NLP extracts insights from electronic health records for better care.


**ä½¿ç”¨å‘é‡ç‚¹ç§¯è¿›è¡Œç›¸ä¼¼åº¦è®¡ç®—**  
- ç›¸ä¼¼åº¦é€šè¿‡å‘é‡çš„**ç‚¹ç§¯**æ¥ç¡®å®šã€‚

- **ç›¸ä¼¼åº¦è®¡ç®—å…¬å¼ï¼š**

$$ \text{similarities} = \mathbf{query} \cdot \mathbf{documents}^T $$

---

 ğŸ“ **å‘é‡ç‚¹ç§¯çš„æ•°å­¦æ„ä¹‰**

**å‘é‡ç‚¹ç§¯çš„å®šä¹‰**

ä¸¤ä¸ªå‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$ çš„**ç‚¹ç§¯**ï¼Œåœ¨æ•°å­¦ä¸Šå®šä¹‰ä¸ºï¼š

$$ \mathbf{a} \cdot \mathbf{b} = \sum_{i=1}^{n} a_i b_i $$

---

**ä¸ä½™å¼¦ç›¸ä¼¼åº¦çš„å…³ç³»**

**ç‚¹ç§¯**ä¸**ä½™å¼¦ç›¸ä¼¼åº¦**ä¹Ÿæœ‰å…³ç³»ï¼Œéµå¾ªä»¥ä¸‹æ€§è´¨ï¼š

$$ \mathbf{a} \cdot \mathbf{b} = \|\mathbf{a}\| \|\mathbf{b}\| \cos \theta $$

å…¶ä¸­ï¼š  
- $\|\mathbf{a}\|$ å’Œ $\|\mathbf{b}\|$ è¡¨ç¤ºå‘é‡ $\mathbf{a}$ å’Œ $\mathbf{b}$ çš„**æ¨¡é•¿**ï¼ˆ**èŒƒæ•°ï¼Œ** ç‰¹æŒ‡æ¬§å‡ é‡Œå¾—èŒƒæ•°ï¼‰ã€‚  
- $\theta$ æ˜¯ä¸¤å‘é‡ä¹‹é—´çš„**å¤¹è§’**ã€‚  
- $\cos \theta$ è¡¨ç¤ºä¸¤å‘é‡ä¹‹é—´çš„**ä½™å¼¦ç›¸ä¼¼åº¦**ã€‚

---

**ğŸ” å‘é‡ç‚¹ç§¯åœ¨ç›¸ä¼¼åº¦è®¡ç®—ä¸­çš„è§£é‡Š**

å½“**ç‚¹ç§¯å€¼å¾ˆå¤§**ï¼ˆå³ä¸€ä¸ªè¾ƒå¤§çš„æ­£å€¼ï¼‰æ—¶ï¼š  
- ä¸¤ä¸ªå‘é‡çš„**æ¨¡é•¿** ($\|\mathbf{a}\|$ å’Œ $\|\mathbf{b}\|$) å¾ˆå¤§ã€‚  
- ä¸¤ä¸ªå‘é‡ä¹‹é—´çš„**å¤¹è§’** ($\theta$) å¾ˆå°ï¼ˆ**$\cos \theta$ æ¥è¿‘ 1**ï¼‰ã€‚

è¿™è¡¨ç¤ºä¸¤ä¸ªå‘é‡æŒ‡å‘**ç›¸ä¼¼çš„æ–¹å‘**ï¼Œå¹¶ä¸”**è¯­ä¹‰ä¸Šæ›´ä¸ºç›¸ä¼¼**ï¼Œç‰¹åˆ«æ˜¯å½“å®ƒä»¬çš„æ¨¡é•¿ä¹Ÿè¾ƒå¤§æ—¶ã€‚

---

ğŸ“ **å‘é‡çš„æ¨¡é•¿ï¼ˆèŒƒæ•°ï¼‰è®¡ç®—**

**æ¬§å‡ é‡Œå¾—èŒƒæ•°çš„å®šä¹‰**

å¯¹äºä¸€ä¸ªå‘é‡ $\mathbf{a} = [a_1, a_2, \ldots, a_n]$ï¼Œå…¶**æ¬§å‡ é‡Œå¾—èŒƒæ•°** $\|\mathbf{a}\|$ çš„è®¡ç®—æ–¹å¼ä¸ºï¼š

$$ \|\mathbf{a}\| = \sqrt{a_1^2 + a_2^2 + \cdots + a_n^2} $$

è¿™ä¸ª**æ¨¡é•¿**è¡¨ç¤ºå‘é‡åœ¨å¤šç»´ç©ºé—´ä¸­çš„**é•¿åº¦**æˆ–**å¤§å°**ã€‚

---

ç†è§£è¿™äº›æ•°å­¦åŸºç¡€æœ‰åŠ©äºç¡®ä¿ç²¾ç¡®çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼Œè¿›è€Œåœ¨**è¯­ä¹‰æœç´¢**ã€**æ£€ç´¢ç³»ç»Ÿ**å’Œ**æ¨èå¼•æ“**ç­‰ä»»åŠ¡ä¸­æé«˜æ€§èƒ½ã€‚ ğŸš€


```python
import numpy as np


def search_similar_documents(q, docs, hf_embeddings):
    """
    Search for the most relevant documents based on a query using text embeddings.

    Args:
        q (str): The query string for which relevant documents are to be found.
        docs (list of str): A list of document strings to compare against the query.
        hf_embeddings: An embedding model object with `embed_query` and `embed_documents` methods.

    Returns:
        tuple:
            - embedded_query (numpy.ndarray): The embedding vector of the query.
            - embedded_documents (numpy.ndarray): The embedding matrix of the documents.
    """
    # Embed the query and documents using the embedding model
    embedded_query = hf_embeddings.embed_query(q)
    embedded_documents = hf_embeddings.embed_documents(docs)

    # Calculate similarity scores using dot product and normalize with the magnitudes
    query_norm = np.linalg.norm(embedded_query)
    document_norms = np.linalg.norm(embedded_documents, axis=1)

    # Calculate cosine similarity: dot product / (query norm * document norm)
    similarity_scores = (embedded_query @ embedded_documents.T) / (query_norm * document_norms)

    # Sort documents by similarity scores in descending order
    sorted_idx = similarity_scores.argsort()[::-1]

    # Display the results
    print(f"[Query] {q}\n" + "=" * 40)
    for i, idx in enumerate(sorted_idx):
        print(f"[{i}] {docs[idx]}")
        print()

    # Return embeddings for potential further processing or analysis
    return embedded_query, embedded_documents

```

## åµŒå…¥å¯è§†åŒ–ï¼ˆPCAï¼‰

ä¸ºäº†å¯è§†åŒ–çš„ç›®çš„ï¼Œå‡å°‘åµŒå…¥çš„ç»´åº¦ã€‚
è¿™æ®µä»£ç ä½¿ç”¨ **ä¸»æˆåˆ†åˆ†æï¼ˆPCAï¼‰** å°†é«˜ç»´åµŒå…¥å‘é‡é™è‡³ **äºŒç»´**ã€‚
ç”Ÿæˆçš„ **äºŒç»´ç‚¹** åœ¨æ•£ç‚¹å›¾ä¸­æ˜¾ç¤ºï¼Œæ¯ä¸ªç‚¹éƒ½æ ‡æœ‰å¯¹åº”æ–‡æ¡£çš„æ ‡ç­¾ã€‚

é«˜ç»´åµŒå…¥å‘é‡ç›´æ¥è¿›è¡Œè§£è¯»å’Œåˆ†æéå¸¸å…·æœ‰æŒ‘æˆ˜æ€§ã€‚é€šè¿‡å°†å®ƒä»¬é™åˆ°äºŒç»´ï¼Œæˆ‘ä»¬å¯ä»¥ï¼š

- **å¯è§†åŒ–æ¢ç´¢åµŒå…¥ä¹‹é—´çš„å…³ç³»**ï¼ˆä¾‹å¦‚ï¼Œèšç±»ã€åˆ†ç»„ï¼‰ã€‚
- **è¯†åˆ«æ•°æ®ä¸­çš„æ¨¡å¼æˆ–å¼‚å¸¸**ï¼Œè¿™äº›æ¨¡å¼åœ¨é«˜ç»´ç©ºé—´ä¸­å¯èƒ½ä¸é‚£ä¹ˆæ˜¾çœ¼ã€‚
- **æé«˜å¯è§£é‡Šæ€§**ï¼Œä½¿æ•°æ®æ›´åŠ æ˜“äºäººç±»åˆ†æå’Œå†³ç­–ã€‚


```python
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
import numpy as np

# Combine documents and query for PCA
all_vectors = np.vstack([docs_vector, query_vector])  # Stack query vector with docs
pca = PCA(n_components=2)
reduced_vectors = pca.fit_transform(all_vectors)

# Separate reduced vectors for documents and query
doc_vectors_2d = reduced_vectors[:-1]  # All but the last point (documents)
query_vector_2d = reduced_vectors[-1]  # Last point (query)

# Plot the reduced vectors
plt.scatter(doc_vectors_2d[:, 0], doc_vectors_2d[:, 1], color="blue", label="Documents")
plt.scatter(query_vector_2d[0], query_vector_2d[1], color="red", label="Query", marker="x", s=300,)

# Annotate document points
for i, doc in enumerate(documents):
    plt.text(doc_vectors_2d[i, 0], doc_vectors_2d[i, 1], doc, fontsize=8)

# Add plot details
plt.title("2D Visualization of Embedding Vectors with Query")
plt.xlabel("PCA Dimension 1")
plt.ylabel("PCA Dimension 2")
plt.legend()
plt.show()
```


    
![png](07-Embedding_files/07-Embedding_11_0.png)
    





```python
from langchain_huggingface.embeddings import HuggingFaceEmbeddings

model_name = "../DataCollection/officials/bge-large-zh-v1.5"

hf_embeddings = HuggingFaceEmbeddings(
    model_name=model_name,
    model_kwargs={"device": 'cuda:1'},  # mps, cuda, cpu
    encode_kwargs={"normalize_embeddings": True},
)
```


```python
vector = hf_embeddings.embed_query("Please tell me more about LangChain.")
print(len(vector))

vector = hf_embeddings.embed_documents([
    "Hi, nice to meet you.",
    "LangChain simplifies the process of building applications with large language models.",
    "The LangChain English tutorial is structured based on LangChain's official documentation, cookbook, and various practical examples to help users utilize LangChain more easily and effectively.",
    "LangChain simplifies the process of building applications with large-scale language models.",
    "Retrieval-Augmented Generation (RAG) is an effective technique for improving AI responses.",
])
print(len(vector))
print([len(item) for item in vector])
```

    1024
    5
    [1024, 1024, 1024, 1024, 1024]


# CacheBackedEmbeddings

åµŒå…¥å€¼å¯ä»¥å­˜å‚¨æˆ–ä¸´æ—¶ç¼“å­˜ï¼Œä»¥é¿å…é‡æ–°è®¡ç®—ã€‚

ç¼“å­˜åµŒå…¥å€¼å¯ä»¥ä½¿ç”¨ `CacheBackedEmbeddings` æ¥å®Œæˆã€‚ç¼“å­˜æ”¯æŒçš„åµŒå…¥å™¨æ˜¯å¯¹åµŒå…¥å™¨çš„åŒ…è£…ï¼Œå®ƒå°†åµŒå…¥å­˜å‚¨åœ¨ä¸€ä¸ªé”®å€¼å­˜å‚¨ä¸­ã€‚æ–‡æœ¬è¢«å“ˆå¸Œå¤„ç†ï¼Œå“ˆå¸Œå€¼ä½œä¸ºç¼“å­˜ä¸­çš„é”®ã€‚


```python
import os

os.makedirs("./cache/", exist_ok=True)
print(os.path.exists("./cache/"))  # Check if the directory exists
print(os.access("./cache/", os.W_OK))  # Check if the directory is writable
```

    True
    True


## ä½¿ç”¨åµŒå…¥ä¸æœ¬åœ°æ–‡ä»¶å­˜å‚¨ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰

åˆå§‹åŒ– `CacheBackedEmbeddings` çš„ä¸»è¦æ”¯æŒæ–¹æ³•æ˜¯ `from_bytes_store`ã€‚

å®ƒæ¥å—ä»¥ä¸‹å‚æ•°ï¼š

- `underlying_embeddings`: ç”¨äºç”ŸæˆåµŒå…¥çš„åµŒå…¥å™¨ã€‚
- `document_embedding_cache`: ç”¨äºç¼“å­˜æ–‡æ¡£åµŒå…¥çš„ `ByteStore` å®ç°ä¹‹ä¸€ã€‚
- `namespace`: ï¼ˆå¯é€‰ï¼Œé»˜è®¤å€¼ä¸º `""`ï¼‰å‘½åç©ºé—´ç”¨äºæ–‡æ¡£ç¼“å­˜ã€‚è¿™æ ·å¯ä»¥é¿å…ä¸å…¶ä»–ç¼“å­˜å‘ç”Ÿå†²çªã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°†å…¶è®¾ç½®ä¸ºæ­£åœ¨ä½¿ç”¨çš„åµŒå…¥æ¨¡å‹çš„åç§°ã€‚

**æ³¨æ„**ï¼šè®¾ç½® `namespace` å‚æ•°éå¸¸é‡è¦ï¼Œä»¥é¿å…åœ¨ä½¿ç”¨ä¸åŒçš„åµŒå…¥æ¨¡å‹å¯¹ç›¸åŒæ–‡æœ¬è¿›è¡ŒåµŒå…¥æ—¶å‘ç”Ÿå†²çªã€‚

é¦–å…ˆï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä½¿ç”¨æœ¬åœ°æ–‡ä»¶ç³»ç»Ÿå­˜å‚¨åµŒå…¥ï¼Œå¹¶ä½¿ç”¨ FAISS å‘é‡å­˜å‚¨è¿›è¡Œæ£€ç´¢çš„ä¾‹å­ã€‚


```python
from langchain.storage import LocalFileStore
from langchain_openai import OpenAIEmbeddings
from langchain.embeddings import CacheBackedEmbeddings
from langchain_community.vectorstores.faiss import FAISS

# Configure basic embeddings using OpenAI embeddings
underlying_embeddings = OpenAIEmbeddings(
	model="bge-m3",
	base_url='http://localhost:9997/v1',
	api_key='cannot be empty',
	# dimensions=1024,
)

# Set up a local file storage
store = LocalFileStore("./cache/")

# Create embeddings with caching support
cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings=underlying_embeddings, 
    document_embedding_cache=store, 
    namespace=underlying_embeddings.model, # Create a cache-backed embedder using the base embedding and storage
)
```

åœ¨ embedding ä¹‹å‰ cache æ˜¯ç©ºçš„


```python
list(store.yield_keys())
```




    []



åŠ è½½æ–‡æ¡£ï¼Œå°†å…¶æ‹†åˆ†ä¸ºå¤šä¸ªå—ï¼Œå¯¹æ¯ä¸ªå—è¿›è¡ŒåµŒå…¥ï¼Œå¹¶å°†åµŒå…¥åŠ è½½åˆ°å‘é‡å­˜å‚¨ä¸­ã€‚


```python
from langchain.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter

raw_documents = TextLoader("./data/appendix-keywords.txt", encoding="utf-8").load()
text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
documents = text_splitter.split_documents(raw_documents)
```

ä»æ–‡æ¡£åˆ›å»º FAISS æ•°æ®åº“ã€‚


```python
%time db = FAISS.from_documents(documents, cached_embedder)
```

    CPU times: user 375 ms, sys: 43 ms, total: 418 ms
    Wall time: 809 ms


å¦‚æœæˆ‘ä»¬å°è¯•å†æ¬¡åˆ›å»ºå‘é‡å­˜å‚¨ï¼Œå®ƒä¼šæ›´å¿«ï¼Œå› ä¸ºæ— éœ€é‡æ–°è®¡ç®—ä»»ä½•åµŒå…¥ã€‚


```python
%time db2 = FAISS.from_documents(documents, cached_embedder)
```

    CPU times: user 13.7 ms, sys: 1.04 ms, total: 14.7 ms
    Wall time: 13.8 ms



```python
list(store.yield_keys())[:5]
```




    ['bge-m34b802135-9b69-54ac-835f-f31f0a8f73cf',
     'bge-m34fd4987e-f5b6-52f8-91e2-886802754643',
     'bge-m3229c1600-8452-5938-b611-45db25315327',
     'bge-m3fed9c955-3b6d-5ce9-b7d2-235f35d18610',
     'bge-m39668cb63-4ad2-528c-9bf2-aecbfa54e1cd']



## ä½¿ç”¨ `InMemoryByteStore`ï¼ˆéæŒä¹…åŒ–ï¼‰

è¦ä½¿ç”¨ä¸åŒçš„ `ByteStore`ï¼Œåªéœ€åœ¨åˆ›å»º `CacheBackedEmbeddings` æ—¶æŒ‡å®šæ‰€éœ€çš„ `ByteStore`ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨éæŒä¹…åŒ–çš„ `InMemoryByteStore` åˆ›å»ºç›¸åŒç¼“å­˜åµŒå…¥å¯¹è±¡çš„ç¤ºä¾‹ã€‚


```python
from langchain.embeddings import CacheBackedEmbeddings
from langchain.storage import InMemoryByteStore

# Create an in-memory byte store
store = InMemoryByteStore()

underlying_embeddings = OpenAIEmbeddings(
	model="bge-m3",
	base_url='http://localhost:9997/v1',
	api_key='cannot be empty',
	# dimensions=1024,
)

cached_embedder = CacheBackedEmbeddings.from_bytes_store(
    underlying_embeddings, store, namespace=underlying_embeddings.model
)
```


```python
%time db = FAISS.from_documents(documents, cached_embedder)  
list(store.yield_keys())[:5]
```

    CPU times: user 6.92 ms, sys: 905 Î¼s, total: 7.82 ms
    Wall time: 7.34 ms





    ['bge-m34b802135-9b69-54ac-835f-f31f0a8f73cf',
     'bge-m34fd4987e-f5b6-52f8-91e2-886802754643',
     'bge-m3229c1600-8452-5938-b611-45db25315327',
     'bge-m3fed9c955-3b6d-5ce9-b7d2-235f35d18610',
     'bge-m39668cb63-4ad2-528c-9bf2-aecbfa54e1cd']


