{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 理解 RAG 的基本结构\n",
    "\n",
    "RAG（检索增强生成）流程中的预处理阶段涉及四个步骤，用于加载、拆分、嵌入和存储文档到向量数据库（Vector DB）。\n",
    "\n",
    "1. 预处理 ## 步骤 1 到 4\n",
    "- **步骤 1：文档加载**：加载文档内容。\n",
    "- **步骤 2：文本拆分**：根据特定标准将文档拆分成多个块。\n",
    "- **步骤 3：嵌入**：为拆分后的文档块生成嵌入，并为存储做准备。\n",
    "- **步骤 4：向量数据库存储**：将生成的嵌入存储到向量数据库中。\n",
    "\n",
    "以上可以称为 Indexing。一个收集数据并对其进行索引的管道。这个过程通常是在离线进行的。\n",
    "\n",
    "2. RAG 执行（运行时） - 步骤 5 到 8\n",
    "- **步骤 5：检索器**：定义一个检索器，根据输入查询从数据库中获取结果。检索器使用搜索算法，通常分为**密集型**和**稀疏型**：\n",
    "  - **密集型**：基于相似度的检索。\n",
    "  - **稀疏型**：基于关键词的检索。\n",
    "- **步骤 6：提示生成**：为执行 RAG 创建一个提示。提示中的`context`包含从文档中检索到的内容。通过提示工程，可以指定回答的格式。\n",
    "- **步骤 7：大语言模型（LLM）**：定义使用的大语言模型（例如 GPT-3.5、GPT-4 或 Claude）。\n",
    "- **步骤 8：链式连接**：创建一个链，将提示、大语言模型和输出连接起来。\n",
    "\n",
    "以上可称为 Retrieval and Generation 。实际的RAG链实时处理用户查询，从索引中检索相关数据，并将其传递给模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 基本 pipeline\n",
    "\n",
    "以下是理解 RAG（检索增强生成）基本结构的框架代码。  \n",
    "每个模块的内容可以根据具体场景进行调整，从而允许逐步改进结构以适应文档。  \n",
    "（在每个步骤中，可以应用不同的选项或新技术。）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 1：文档加载**：加载文档内容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages in the document: 1\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load Documents\n",
    "loader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of pages in the document: {len(docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 2：文本拆分**：根据特定标准将文档拆分成多个块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split chunks: 30\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Number of split chunks: {len(split_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 3：嵌入**：为拆分后的文档块生成嵌入，并为存储做准备。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 4：向量数据库存储**：将生成的嵌入存储到向量数据库中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and Save the Database\n",
    "# Create a vector store.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test similarity_search\n",
    "for doc in vectorstore.similarity_search(\"URBAN MOBILITY\", k=1):\n",
    "    print(doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **步骤 5：检索器**：定义一个检索器，根据输入查询从数据库中获取结果。检索器使用搜索算法，通常分为**密集型**和**稀疏型**：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Create Retriever\n",
    "# Search and retrieve information contained in the documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={'k': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.invoke(\"What is the phased implementation timeline for the EU AI Act?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **步骤 6：提示生成**：为执行 RAG 创建一个提示。提示中的`context`包含从文档中检索到的内容。通过提示工程，可以指定回答的格式。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\n\\n#Context: \\n{context}\\n\\n#Question:\\n{question}\\n\\n#Answer:\")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Create Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 7：大语言模型（LLM）**：定义使用的大语言模型（例如 GPT-3.5、GPT-4 或 Claude）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Load LLM\n",
    "llm = ChatOpenAI(\n",
    "\tbase_url='http://localhost:5551/v1',\n",
    "\tapi_key='EMPTY',\n",
    "\tmodel_name='Qwen2.5-7B-Instruct',\n",
    "\ttemperature=0.2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **步骤 8：链式连接**：创建一个链，将提示、大语言模型和输出连接起来。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Chain\n",
    "# Input a query about the document and print the response.\n",
    "question = \"Where has the application of AI in healthcare been confined to so far?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 完整版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Step 1: Load Documents\n",
    "loader = TextLoader(\"data/appendix-keywords.txt\", encoding=\"utf-8\")\n",
    "docs = loader.load()\n",
    "print(f\"Number of pages in the document: {len(docs)}\")\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Number of split chunks: {len(split_documents)}\")\n",
    "\n",
    "# Step 3: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "# Step 4: Create and Save the Database\n",
    "# Create a vector store.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# Step 5: Create Retriever\n",
    "# Search and retrieve information contained in the documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={'k': 1})\n",
    "\n",
    "# Step 6: Create Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "\n",
    "#Context: \n",
    "{context}\n",
    "\n",
    "#Question:\n",
    "{question}\n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Step 7: Load LLM\n",
    "llm = ChatOpenAI(\n",
    "\tbase_url='http://localhost:5551/v1',\n",
    "\tapi_key='EMPTY',\n",
    "\tmodel_name='Qwen2.5-7B-Instruct',\n",
    "\ttemperature=0.2,\n",
    ")\n",
    "\n",
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run Chain\n",
    "# Input a query about the document and print the response.\n",
    "question = \"Where has the application of AI in healthcare been confined to so far?\"\n",
    "response = chain.invoke(question)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多轮对话\n",
    "\n",
    "创建一个记住之前对话的链条, 将对话历史添加到 chain\n",
    "\n",
    "- 使用 `MessagesPlaceholder` 来包含对话历史。\n",
    "- 定义一个提示语，接收用户输入的问题。\n",
    "- 创建一个 `ChatOpenAI` 实例，使用 OpenAI 的 `ChatGPT` 模型。\n",
    "- 通过将提示语、语言模型和输出解析器连接起来，构建链条。\n",
    "- 使用 `StrOutputParser` 将模型的输出转换为字符串。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础问答框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# Defining the prompt\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"You are a Question-Answering chatbot. Please provide an answer to the given question.\",\n",
    "        ),\n",
    "        # Please use the key 'chat_history' for conversation history without changing it if possible!\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"human\", \"#Question:\\n{question}\"),  # Use user input as a variable\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Generating an LLM\n",
    "llm = ChatOpenAI(\n",
    "\tbase_url='http://localhost:5551/v1',\n",
    "\tapi_key='EMPTY',\n",
    "\tmodel_name='Qwen2.5-7B-Instruct',\n",
    "\ttemperature=0.2,\n",
    ")\n",
    "\n",
    "# Creating a regular Chain\n",
    "chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建可管理对话历史的 chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建一个记录对话的链条 (chain_with_history)\n",
    "\n",
    "- 创建一个字典来存储会话记录。\n",
    "- 定义一个函数，根据会话 ID 检索会话记录。如果会话 ID 不在存储中，创建一个新的 `ChatMessageHistory` 对象。\n",
    "- 创建一个 `RunnableWithMessageHistory` 对象来管理对话历史。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store session records\n",
    "store = {}\n",
    "\n",
    "# Function to retrieve session records based on session ID\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[Conversation Session ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # If the session ID is not in the store\n",
    "        # Create a new ChatMessageHistory object and save it to the store\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  # Return the session history for the corresponding session ID\n",
    "\n",
    "\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # Function to retrieve session history\n",
    "    input_messages_key=\"question\",  # Key for the template variable that will contain the user's question\n",
    "    history_messages_key=\"chat_history\",  # Key for the history messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多轮 QA 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello Jack! Nice to meet you. How can I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"My name is Jack.\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: abc123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Your name is Jack.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"What is my name?\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human: My name is Jack.\n",
      "AI: Hello Jack! Nice to meet you. How can I assist you today?\n",
      "Human: What is my name?\n",
      "AI: Your name is Jack.\n"
     ]
    }
   ],
   "source": [
    "print(store['abc123'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 带多轮对话的 RAG 框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础问答框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of split chunks: 171\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PDFPlumberLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from operator import itemgetter\n",
    "\n",
    "# Step 1: Load Documents\n",
    "loader = PDFPlumberLoader(\"data/A European Approach to Artificial Intelligence - A Policy Perspective.pdf\") \n",
    "docs = loader.load()\n",
    "\n",
    "# Step 2: Split Documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(docs)\n",
    "print(f\"Number of split chunks: {len(split_documents)}\")\n",
    "\n",
    "# Step 3: Generate Embeddings\n",
    "embeddings = OpenAIEmbeddings(\n",
    "\tmodel=\"bge-m3\",\n",
    "\tbase_url='http://localhost:9997/v1',\n",
    "\tapi_key='cannot be empty',\n",
    "\t# dimensions=1024,\n",
    ")\n",
    "\n",
    "# Step 4: Create and Save the Database\n",
    "# Create a vector store.\n",
    "vectorstore = FAISS.from_documents(documents=split_documents, embedding=embeddings)\n",
    "\n",
    "# Step 5: Create Retriever\n",
    "# Search and retrieve information contained in the documents.\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={'k': 1})\n",
    "\n",
    "# Step 6: Create Prompt\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know.\n",
    "\n",
    "#Previous Chat History:\n",
    "{chat_history}\n",
    "\n",
    "#Question: \n",
    "{question} \n",
    "\n",
    "#Context: \n",
    "{context} \n",
    "\n",
    "#Answer:\"\"\"\n",
    ")\n",
    "\n",
    "# Step 7: Load LLM\n",
    "llm = ChatOpenAI(\n",
    "\tbase_url='http://localhost:5551/v1',\n",
    "\tapi_key='EMPTY',\n",
    "\tmodel_name='Qwen2.5-7B-Instruct',\n",
    "\ttemperature=0.2,\n",
    ")\n",
    "\n",
    "# Step 8: Create Chain\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": itemgetter(\"question\") | retriever,\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\"),\n",
    "    }\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建可管理对话历史的 chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store session records\n",
    "store = {}\n",
    "\n",
    "# Function to retrieve session records based on session ID\n",
    "def get_session_history(session_ids):\n",
    "    print(f\"[Conversation Session ID]: {session_ids}\")\n",
    "    if session_ids not in store:  # If the session ID is not in the store\n",
    "        # Create a new ChatMessageHistory object and save it to the store\n",
    "        store[session_ids] = ChatMessageHistory()\n",
    "    return store[session_ids]  \n",
    "\n",
    "# Create a RAG chain that records conversations\n",
    "rag_with_history = RunnableWithMessageHistory(\n",
    "    chain,\n",
    "    get_session_history,  # Function to retrieve session history\n",
    "    input_messages_key=\"question\",  # Key for the template variable that will contain the user's question\n",
    "    history_messages_key=\"chat_history\",  # Key for the history messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 多轮 QA 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, the three key components necessary to achieve 'trustworthy AI' in the European approach to AI policy are not explicitly stated. The context provided seems to be about improving public services and creating a secure, trusted data space, but it does not directly address the components of trustworthy AI. To provide an accurate answer, I would need more specific information from the document.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"What are the three key components necessary to achieve 'trustworthy AI' in the European approach to AI policy?\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conversation Session ID]: rag123\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'No se especifican los tres componentes clave necesarios para lograr la \"IA confiable\" en la aproximación europea a la política de IA en el contexto proporcionado. El contexto parece estar sobre mejorar los servicios públicos y crear un espacio de datos seguro y confiable, pero no aborda directamente los componentes de la IA confiable. Para proporcionar una respuesta precisa, necesitaría más información específica del documento.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_with_history.invoke(\n",
    "    # Input question\n",
    "    {\"question\": \"Please translate the previous answer into Spanish.\"},\n",
    "    # Record the conversation based on the session ID.\n",
    "    config={\"configurable\": {\"session_id\": \"rag123\"}},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xp-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
